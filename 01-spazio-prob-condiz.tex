\input{preambolo_comune}

% --- Titolo ---
\title{Spazio Probabilità e Probabilità Condizionata}
\author{Alessandro Amella}
\date{\today}

\begin{document}

\maketitle
\tableofcontents
\newpage

\section{Probabilità Condizionata}

\subsection{Definizione e intuizione}

Spesso, nel calcolo delle probabilità, ci troviamo a valutare la probabilità di un evento $A$ quando siamo già a conoscenza che un altro evento $B$ si è verificato. Questa nuova informazione (il fatto che $B$ sia accaduto) può cambiare la nostra valutazione della probabilità di $A$.

\begin{definition}
Siano $A$ e $B$ due eventi, con $\Pc(B) > 0$. La \textbf{probabilità condizionata} (o condizionale) di $A$ dato $B$ è definita come:
\[ \Pcond{A}{B} = \frac{\Pc(A \capSet B)}{\Pc(B)} \]
\end{definition}

% \textcolor{blue}{Intuizione:}
\begin{remark}
Sapere che $B$ si è verificato restringe il nostro "universo" dei possibili esiti. Non siamo più interessati a tutto $\OmegaSet$, ma solo agli esiti contenuti in $B$. All'interno di questo nuovo universo ridotto ($B$), vogliamo vedere qual è la "porzione" occupata dagli esiti che sono anche in $A$. Questa porzione è $A \capSet B$. La formula normalizza questa misura dividendola per la probabilità del nuovo universo, $\Pc(B)$.
\end{remark}

\begin{example}[Dado a 6 facce, probabilità uniforme]
Lanciamo un dado a 6 facce non truccato.
Sia $\OmegaSet = \{1, 2, 3, 4, 5, 6\}$. Ogni esito ha probabilità $1/6$.
Sia $A = \text{"esce un numero maggiore o uguale a 3"} = \{3, 4, 5, 6\}$.
Sia $B = \text{"esce un numero pari"} = \{2, 4, 6\}$.
Vogliamo calcolare $\Pcond{A}{B}$, la probabilità che esca un numero $\ge 3$ sapendo che è uscito un numero pari.

\begin{itemize}
    \item $\Pc(B) = \Pc(\{2, 4, 6\}) = 3/6 = 1/2$.
    \item $A \capSet B = \{3, 4, 5, 6\} \capSet \{2, 4, 6\} = \{4, 6\}$.
    \item $\Pc(A \capSet B) = \Pc(\{4, 6\}) = 2/6 = 1/3$.
\end{itemize}
Quindi:
\[ \Pcond{A}{B} = \frac{\Pc(A \capSet B)}{\Pc(B)} = \frac{1/3}{1/2} = \frac{2}{3} \]
\begin{remark}
Se sappiamo che è uscito un numero pari, i soli esiti possibili sono $\{2, 4, 6\}$. Di questi, quelli favorevoli ad $A$ (cioè $\ge 3$) sono $\{4, 6\}$. Quindi, 2 casi favorevoli su 3 possibili. Questo coincide con $2/3$.
\end{remark}
\end{example}

\begin{example}[Dado truccato a 4 facce, probabilità non uniforme]
Lanciamo un dado truccato a 4 facce, $\OmegaSet = \{1, 2, 3, 4\}$.
Le probabilità sono:
$\Pc(\{1\}) = 8/15$, $\Pc(\{2\}) = 4/15$, $\Pc(\{3\}) = 2/15$, $\Pc(\{4\}) = 1/15$.
Sia $A = \text{"esce un numero maggiore o uguale a 3"} = \{3, 4\}$.
Sia $B = \text{"esce un numero pari"} = \{2, 4\}$.
Vogliamo calcolare $\Pcond{A}{B}$.

\begin{itemize}
    \item $\Pc(B) = \Pc(\{2\}) + \Pc(\{4\}) = 4/15 + 1/15 = 5/15 = 1/3$.
    \item $A \capSet B = \{3, 4\} \capSet \{2, 4\} = \{4\}$.
    \item $\Pc(A \capSet B) = \Pc(\{4\}) = 1/15$.
\end{itemize}
Quindi:
\[ \Pcond{A}{B} = \frac{\Pc(A \capSet B)}{\Pc(B)} = \frac{1/15}{1/3} = \frac{1}{15} \cdot 3 = \frac{3}{15} = \frac{1}{5} \]
\begin{remark}
Se sappiamo che è uscito un numero pari (evento $B$), i "veri" casi possibili sono 2 e 4. L'evento $A$ (numero $\ge 3$) si verifica solo se esce 4. Dobbiamo rapportare la probabilità del "ver
\end{remark}
Questo conferma la formula generale.
\end{example}

\begin{exercise}
Considera l'esperimento del lancio di due dadi non truccati.
Sia $S$ la somma dei punteggi.
Sia $A = \text{"la somma dei punteggi è 7"}$.
Sia $B = \text{"il primo dado ha dato come risultato 3"}$.
Calcola $\Pcond{A}{B}$.
\end{exercise}
\textbf{Soluzione (traccia):}
$\OmegaSet$ ha $6 \times 6 = 36$ esiti equiprobabili.
$B = \{(3,1), (3,2), (3,3), (3,4), (3,5), (3,6)\}$, quindi $\Pc(B) = 6/36 = 1/6$.
$A = \{(1,6), (2,5), (3,4), (4,3), (5,2), (6,1)\}$.
$A \capSet B = \{(3,4)\}$, quindi $\Pc(A \capSet B) = 1/36$.
$\Pcond{A}{B} = \frac{1/36}{1/6} = 1/6$.
\begin{remark}
Se il primo dado è 3, per fare somma 7 il secondo dado deve essere 4. La probabilità che il secondo dado sia 4 è $1/6$.
\end{remark}

\subsection{Proprietà della probabilità condizionata}
Fissato un evento $B$ con $\Pc(B)>0$, la funzione $\Pcond{\cdot}{B}$ (che mappa un evento $A$ in $\Pcond{A}{B}$) è essa stessa una misura di probabilità. Ciò significa che soddisfa gli assiomi di Kolmogorov.

\begin{theorem}
Sia $B$ un evento tale che $\Pc(B)>0$. Valgono le seguenti proprietà:
\begin{enumerate}[label=\Roman*)]
    \item Per ciascun evento $A \subseteq \OmegaSet$, si ha $0 \le \Pcond{A}{B} \le 1$.
    \item $\Pcond{\OmegaSet}{B} = 1$.
    \item Se $A_1, A_2, \dots, A_n, \dots$ è una successione di eventi a due a due disgiunti, allora
    \[ \Pcond{\bigcup_{n=1}^\infty A_n}{B} = \sum_{n=1}^\infty \Pcond{A_n}{B} \quad (\sigma\text{-additività}) \]
    Da cui derivano:
    \item $\Pcond{\emptyset}{B} = 0$.
    \item Se $A_1, A_2$ sono disgiunti: $\Pcond{A_1 \cupSet A_2}{B} = \Pcond{A_1}{B} + \Pcond{A_2}{B}$ (additività finita).
    \item $\Pcond{\compSet{A}}{B} = 1 - \Pcond{A}{B}$.
    \item Se $A_1 \subseteq A_2$, allora $\Pcond{A_1}{B} \le \Pcond{A_2}{B}$ (monotonia).
\end{enumerate}
\end{theorem}

\begin{proof}
Dato che $A \capSet B \subseteq B$, per la monotonia della probabilità $\Pc$ (non condizionata), si ha $\Pc(A \capSet B) \le \Pc(B)$.
Poiché $\Pc(B)>0$, possiamo dividere per $\Pc(B)$:
\[ \frac{\Pc(A \capSet B)}{\Pc(B)} \le 1 \implies \Pcond{A}{B} \le 1 \]
Inoltre, $\Pc(A \capSet B) \ge 0$, quindi $\Pcond{A}{B} = \frac{\Pc(A \capSet B)}{\Pc(B)} \ge 0$.
Quindi $0 \le \Pcond{A}{B} \le 1$. Le altre proprietà si dimostrano in modo analogo, usando la definition di probabilità condizionata e le proprietà di $\Pc$.
\end{proof}

\begin{remark}
In generale, $\Pcond{A}{B} \neq \Pcond{B}{A}$.
Ad example, la probabilità che una persona abbia la febbre sapendo che ha l'influenza è alta. Ma la probabilità che una persona abbia l'influenza sapendo che ha la febbre potrebbe essere più bassa (la febbre può avere altre cause).
La relazione tra $\Pcond{A}{B}$ e $\Pcond{B}{A}$ è data dalla Formula di Bayes, che vedremo più avanti.
\end{remark}


\section{Utilizzo della Probabilità Condizionata}

\subsection{Regola della catena (o formula della probabilità composta)}
Dalla definition di probabilità condizionata $\Pcond{A}{B} = \frac{\Pc(A \capSet B)}{\Pc(B)}$, se $\Pc(B)>0$, possiamo ricavare una formula per calcolare $\Pc(A \capSet B)$:
\[ \Pc(A \capSet B) = \Pcond{A}{B} \Pc(B) \]
Questa formula è estremamente utile, specialmente in esperimenti che avvengono in più stadi.

\begin{theorem}[Regola della catena per due eventi]
Siano $A$ e $B$ due eventi. Se $\Pc(B) > 0$, allora:
\[ \Pc(A \capSet B) = \Pcond{A}{B} \Pc(B) \]
Analogamente, se $\Pc(A) > 0$, allora:
\[ \Pc(A \capSet B) = \Pcond{B}{A} \Pc(A) \]
\end{theorem}

Questa regola si può generalizzare a più eventi.
\begin{theorem}[Regola della catena per tre eventi]
Per tre eventi $A_1, A_2, A_3$, se $\Pc(A_1 \capSet A_2) > 0$, allora:
\[ \Pc(A_1 \capSet A_2 \capSet A_3) = \Pcond{A_3}{A_1 \capSet A_2} \Pcond{A_2}{A_1} \Pc(A_1) \]
\end{theorem}
\begin{proof}
$\Pc(A_1 \capSet A_2 \capSet A_3) = \Pc((A_1 \capSet A_2) \capSet A_3)$.
Chiamiamo $C = A_1 \capSet A_2$. Allora $\Pc(C \capSet A_3) = \Pcond{A_3}{C} \Pc(C)$.
Sostituendo $C$: $\Pcond{A_3}{A_1 \capSet A_2} \Pc(A_1 \capSet A_2)$.
Ora applichiamo la regola della catena a $\Pc(A_1 \capSet A_2) = \Pcond{A_2}{A_1} \Pc(A_1)$.
Mettendo insieme: $\Pcond{A_3}{A_1 \capSet A_2} \Pcond{A_2}{A_1} \Pc(A_1)$.
\end{proof}

\begin{example}[Estrazioni da un'urna senza reimmissione]
Un'urna contiene 3 palline bianche (B) e 2 nere (N). Si eseguono tre estrazioni senza reimmissione.
Qual è la probabilità di estrarre nell'ordine una bianca, una rossa (R) e una nera (N)? Ah, l'example della dispensa dice "una bianca, una rossa e una nera", ma l'urna contiene solo B e N. Correggiamo l'example con i dati della dispensa: 3B, 2N, 1R. Totale 6 palline.
Vogliamo calcolare $\Pc(B_1 \capSet R_2 \capSet N_3)$, dove $B_1$ è "bianca alla prima estrazione", $R_2$ è "rossa alla seconda", $N_3$ è "nera alla terza".

Usiamo la regola della catena:
\[ \Pc(B_1 \capSet R_2 \capSet N_3) = \Pcond{N_3}{B_1 \capSet R_2} \Pcond{R_2}{B_1} \Pc(B_1) \]
Calcoliamo i termini:
\begin{itemize}
    \item $\Pc(B_1)$: All'inizio ci sono 3B, 2N, 1R (tot 6). $\Pc(B_1) = 3/6 = 1/2$.
    \item $\Pcond{R_2}{B_1}$: Se la prima è stata bianca, nell'urna restano 2B, 2N, 1R (tot 5). $\Pcond{R_2}{B_1} = 1/5$.
    \item $\Pcond{N_3}{B_1 \capSet R_2}$: Se la prima è stata bianca e la seconda rossa, nell'urna restano 2B, 2N, 0R (tot 4). $\Pcond{N_3}{B_1 \capSet R_2} = 2/4 = 1/2$.
\end{itemize}
Quindi:
\[ \Pc(B_1 \capSet R_2 \capSet N_3) = \frac{1}{2} \cdot \frac{1}{5} \cdot \frac{1}{2} = \frac{1}{20} \]
(Questo risultato differisce da quello della dispensa $2/4 \cdot 1/5 \cdot 3/6 = 1/20$ perché la probabilità di $N_3$ dato $B_1 \cap R_2$ nella dispensa è $2/4$, mentre il mio example originale aveva solo B e N. Ho adattato all'example della dispensa.)
\end{example}

\subsection{Diagrammi ad albero}
I diagrammi ad albero sono uno strumento visuale molto utile per calcolare probabilità in esperimenti a più stadi, specialmente quando si usa la regola della catena.
\begin{itemize}
    \item Ogni nodo rappresenta un possibile stato o esito parziale. La radice è l'evento certo $\OmegaSet$.
    \item Ogni ramo che parte da un nodo rappresenta un possibile esito del sotto-esperimento successivo.
    \item Ad ogni ramo è associata una probabilità:
        \begin{itemize}
            \item I rami che partono dalla radice hanno probabilità non condizionate (es. $\Pc(A_1)$).
            \item I rami che partono da un nodo successivo (es. da $A_1$) hanno probabilità condizionate all'evento rappresentato dal nodo di partenza (es. $\Pcond{A_2}{A_1}$).
        \end{itemize}
    \item La probabilità di un \textbf{cammino} completo (dalla radice a una foglia) è il prodotto delle probabilità sui rami che compongono quel cammino. Questo è esattamente ciò che dice la regola della catena.
    \item La somma delle probabilità dei rami che escono da un \textit{medesimo nodo} deve fare 1 (purché questi rami rappresentino una partizione degli esiti possibili da quel nodo).
\end{itemize}

\begin{example}[Riprendiamo l'urna con 3B, 2N, 1R, 3 estrazioni senza reimmissione]
Vogliamo $\Pc(B_1 \capSet R_2 \capSet N_3)$.
\begin{itemize}
    \item Radice ($\OmegaSet$)
    \item Ramo 1: $B_1$, prob. $\Pc(B_1) = 3/6$. Arriviamo al nodo $B_1$. (Urna: 2B, 2N, 1R)
    \item Dal nodo $B_1$, Ramo 2: $R_2$, prob. $\Pcond{R_2}{B_1} = 1/5$. Arriviamo al nodo $B_1 \capSet R_2$. (Urna: 2B, 2N, 0R)
    \item Dal nodo $B_1 \capSet R_2$, Ramo 3: $N_3$, prob. $\Pcond{N_3}{B_1 \capSet R_2} = 2/4$. Arriviamo alla foglia $B_1 \capSet R_2 \capSet N_3$.
\end{itemize}
Probabilità del cammino: $(3/6) \cdot (1/5) \cdot (2/4) = 1/20$.

Se volessimo calcolare $\Pc(\text{estrarre esattamente una Nera nelle prime due estrazioni})$, dovremmo identificare i cammini che portano a questo evento (es. $N_1 \capSet B_2$, $B_1 \capSet N_2$, $N_1 \capSet R_2$, $R_1 \capSet N_2$) calcolare la loro probabilità e sommarle (perché sono eventi disgiunti).
\end{example}

\subsection{Partizioni di \texorpdfstring{$\OmegaSet$}{Omega} (Schema di alternative)}
Una partizione è un concetto fondamentale, specialmente per la formula delle probabilità totali.

\begin{definition}
Si dice che $n$ eventi (o sottoinsiemi) $H_1, H_2, \dots, H_n$ di $\OmegaSet$ formano una \textbf{partizione} (o schema di alternative) di $\OmegaSet$ se:
\begin{enumerate}
    \item Gli insiemi $H_1, \dots, H_n$ sono a due a due disgiunti (cioè $H_i \capSet H_j = \emptyset$ per $i \neq j$).
    \item L'unione degli insiemi $H_1, \dots, H_n$ è $\OmegaSet$ (cioè $\bigcup_{i=1}^n H_i = \OmegaSet$).
\end{enumerate}
In breve: gli $H_i$ sono "pezzi" disgiunti che ricoprono interamente $\OmegaSet$.
\end{definition}

\begin{remark}
Se una partizione è costituita da soli due insiemi $H_1$ e $H_2$, allora $H_2$ deve essere il complementare di $H_1$, cioè $H_2 = \compSet{H_1}$.
\end{remark}

\begin{example}[Esercizio 1.2 dalla dispensa]
Ci sono due urne. 
\begin{itemize}
    \item Urna 1 (U1): 2 Rosse (R), 1 Bianca (B)
    \item Urna 2 (U2): 3R, 2B
\end{itemize}

Si lancia una moneta (T=Testa, C=Croce, $\Pc(T)=\Pc(C)=1/2$).
\begin{itemize}
    \item Se esce Testa, si estrae una pallina da U1
    \item Se esce Croce, si estrae una pallina da U2
\end{itemize}

\textbf{Domanda:} Qual è la probabilità che l'esito del lancio sia Testa E la pallina estratta sia Bianca?

\textbf{Soluzione:}
Chiamiamo $T$ l'evento "esce Testa" e $B_{est}$ l'evento "pallina estratta è Bianca". 
Vogliamo calcolare $\Pc(T \capSet B_{est})$.

Usiamo la regola della catena: $\Pc(T \capSet B_{est}) = \Pcond{B_{est}}{T} \Pc(T)$.
\begin{itemize}
    \item $\Pc(T) = 1/2$
    \item $\Pcond{B_{est}}{T}$: Se è uscita Testa, peschiamo da U1 (2R, 1B, tot 3). 
    La probabilità di estrarre Bianca da U1 è $1/3$. 
    Quindi $\Pcond{B_{est}}{T} = 1/3$
\end{itemize}

Dunque, $\Pc(T \capSet B_{est}) = (1/3) \cdot (1/2) = 1/6$.

\textbf{Osservazione:} Qui, gli eventi $T$ ("scelta U1") e $C$ ("scelta U2") formano una partizione dello spazio degli esiti del lancio della moneta. Anche gli eventi $B_{est}$ e $R_{est}$ (pallina estratta Rossa) formano una partizione degli esiti dell'estrazione, \textit{una volta fissata l'urna}.
\end{example}

\begin{exercise}[Ispirato a esame 08 Settembre 2023, Esercizio 1]
Un giocatore inizia con 1 euro. Lancia una moneta bilanciata ($\Pc(T)=\Pc(C)=1/2$).
\begin{itemize}
    \item Se esce Testa (T) vince 1 euro
    \item Se esce Croce (C) perde 1 euro
    \item Il gioco si ferma se arriva a 0 euro (rovina) o a 3 euro (vittoria)
\end{itemize}

\textbf{Domande:}
\begin{enumerate}
    \item Qual è la probabilità di finire in rovina al secondo lancio?
    \item Qual è la probabilità di vincere al terzo lancio?
\end{enumerate}

\textbf{Soluzione (Rovina al secondo lancio):}
Per finire in rovina al secondo lancio, deve perdere al primo (va a 0 euro) e poi... il gioco si è già fermato!

Analizziamo la situazione:
\begin{itemize}
    \item Stato iniziale: 1 euro ($S_0 = 1$)
    \item Primo lancio:
        \begin{itemize}
            \item Se $T_1$ (prob 1/2) $\rightarrow$ 2 euro
            \item Se $C_1$ (prob 1/2) $\rightarrow$ 0 euro (ROVINA). Gioco fermo
        \end{itemize}
\end{itemize}

\textbf{Conclusione:} Non è possibile andare in rovina \textit{esattamente} al secondo lancio partendo da 1 euro, se ci si ferma subito a 0.

\textbf{Soluzione (Vincere al terzo lancio):}
Analizziamo i possibili cammini per vincere al terzo lancio (arrivare a 3 euro):

\begin{itemize}
    \item Stato iniziale: $S_0 = 1$
    \item Primo lancio ($L_1$):
        \begin{itemize}
            \item $C_1$ (perde) $\rightarrow S_1 = 0$. Gioco fermo
            \item $T_1$ (vince) $\rightarrow S_1 = 2$
        \end{itemize}
    \item Se $S_1 = 2$, secondo lancio ($L_2$):
        \begin{itemize}
            \item $C_2$ (perde) $\rightarrow S_2 = 1$
            \item $T_2$ (vince) $\rightarrow S_2 = 3$ (VITTORIA)
        \end{itemize}
\end{itemize}

Possibili sequenze:
\begin{itemize}
    \item $S_0=1 \xrightarrow{T_1} S_1=2 \xrightarrow{C_2} S_2=1 \xrightarrow{T_3} S_3=2$ (Non vince)
    \item $S_0=1 \xrightarrow{T_1} S_1=2 \xrightarrow{T_2} S_2=3$ (Vittoria al secondo lancio)
\end{itemize}

\textbf{Conclusione:} Con la regola di fermarsi a 0 o 3 euro partendo da 1 euro, il gioco non può durare 3 lanci se l'obiettivo è 3 euro. 
\begin{itemize}
    \item O si va a 0 al primo colpo ($C_1$)
    \item O si va a 2 al primo colpo ($T_1$)
        \begin{itemize}
            \item Se si va a 2, o si va a 1 ($T_1 C_2$)
            \item O si va a 3 ($T_1 T_2$, probabilità $1/2 \cdot 1/2 = 1/4$)
        \end{itemize}
    \item A questo punto il gioco è finito
\end{itemize}

\textbf{Nota:} L'esercizio d'esame originale è più complesso perché ci sono due monete e un budget massimo di 5 lanci, fermandosi a 0 euro o quando si hanno 4 euro. La logica dei cammini sull'albero e la regola della catena sono fondamentali lì.
\end{exercise}


\end{document}