\input{preambolo_comune}

\title{How to Esame!!\\
  \large Basato sulle soluzioni di esami passati}
\author{Alessandro Amella, Gemini e Claude}
\date{\today}

\begin{document}

\maketitle
\tableofcontents
\newpage

\chapter{Introduzione}
\section{A cosa serve la Probabilità e la Statistica}
Il Calcolo delle Probabilità è la branca della matematica che si occupa di analizzare e quantificare l'incertezza. Nella vita di tutti i giorni, così come in ambito scientifico, ingegneristico ed economico, ci troviamo costantemente di fronte a situazioni il cui esito non è predicibile con certezza. La probabilità ci fornisce gli strumenti per modellare queste situazioni, assegnare misure numeriche alla possibilità che certi eventi accadano e prendere decisioni informate in condizioni di incertezza.

La Statistica, d'altra parte, si occupa della raccolta, dell'analisi, dell'interpretazione e della presentazione dei dati. Spesso utilizza concetti probabilistici per trarre conclusioni su una popolazione più ampia a partire da un campione limitato di osservazioni (inferenza statistica) o per descrivere le caratteristiche principali di un insieme di dati (statistica descrittiva).

In questo corso, e in particolare in preparazione per l'esame, ci concentreremo sugli aspetti fondamentali del calcolo delle probabilità e su come applicarli per risolvere problemi specifici, come quelli visti nelle tracce d'esame.

\section{Come usare questa guida}
Questa guida è stata pensata per accompagnarti passo passo nello studio del Calcolo delle Probabilità e Statistica, con un focus specifico sulla preparazione per l'esame. Ogni capitolo tratterà un blocco di argomenti teorici, illustrandoli con definizioni precise, proprietà fondamentali e, soprattutto, esempi pratici.

\textbf{Approccio consigliato:}
\begin{enumerate}
    \item \textbf{Leggi la teoria attentamente:} Assicurati di aver compreso le definizioni e i concetti chiave prima di passare agli esempi.
    \item \textbf{Analizza gli esempi svolti:} Non limitarti a leggere la soluzione. Cerca di capire il ragionamento che porta a quella soluzione, identificando i concetti teorici applicati. Prova a rifare gli esempi da solo prima di guardare la soluzione.
    \item \textbf{Confronta con gli esercizi d'esame:} Molti esempi in questa guida saranno direttamente ispirati o tratti dagli esercizi d'esame che hai fornito. Questo ti aiuterà a familiarizzare con le tipologie di problemi e le tecniche risolutive richieste.
    \item \textbf{Fai pratica:} La matematica si impara facendola. Dopo aver studiato un capitolo, prova a risolvere esercizi simili, specialmente quelli delle tracce d'esame.
    \item \textbf{Non aver paura di tornare indietro:} Se un concetto non è chiaro, rileggi la parte teorica o gli esempi precedenti.
\end{enumerate}
L'obiettivo non è solo memorizzare formule, ma capire i principi che stanno dietro e saperli applicare in contesti diversi. In bocca al lupo!

\chapter{Fondamenti di Probabilità}
\label{cap:fondamenti}
In questo capitolo introdurremo i concetti di base del calcolo delle probabilità, che sono il mattone fondamentale per tutto ciò che seguirà.

\section{Spazio Campionario (\texorpdfstring{$\Omega$}{Omega}) ed Eventi}

\begin{definition}[Esperimento Aleatorio]
Un \textbf{esperimento aleatorio} è un processo il cui esito non può essere previsto con certezza prima che venga eseguito, anche se tutte le possibili realizzazioni sono note.
\end{definition}

\begin{example}
Esempi di esperimenti aleatori sono:
\begin{itemize}
    \item Il lancio di una moneta (possibili esiti: Testa, Croce).
    \item Il lancio di un dado a sei facce (possibili esiti: 1, 2, 3, 4, 5, 6).
    \item L'estrazione di una carta da un mazzo.
    \item La scelta di uno studente da una classe per un'interrogazione.
\end{itemize}
\end{example}

\begin{definition}[Spazio Campionario \texorpdfstring{$\Omega$}{Omega}]
Lo \textbf{spazio campionario}, indicato con $\Omega$, è l'insieme di tutti i possibili esiti (o risultati elementari) di un esperimento aleatorio.
\end{definition}

\begin{example}
\begin{enumerate}
    \item Lancio di una moneta: $\Omega = \{T, C\}$ (dove T=Testa, C=Croce).
    \item Lancio di un dado: $\Omega = \{1, 2, 3, 4, 5, 6\}$.
    \item Lancio di due monete (o una moneta due volte): $\Omega = \{(T,T), (T,C), (C,T), (C,C)\}$. Nota che l'ordine può essere importante.
    \item \textbf{Esercizio 2, 08/09/2023 (Scelta gruppo):} Una classe di 10 studenti viene suddivisa in due sottogruppi di 5. Il primo va a Firenze, il secondo a Roma. Se definiamo un esito come la composizione del gruppo che va a Roma (o Firenze), $\Omega$ è l'insieme di tutte le possibili combinazioni di 5 studenti scelti da 10. Il numero di tali esiti è $\binom{10}{5}$.
\end{enumerate}
\end{example}

\begin{definition}[Evento]
Un \textbf{evento} $A$ è un qualsiasi sottoinsieme dello spazio campionario $\Omega$ (cioè $A \subseteq \Omega$). Un evento si dice \textbf{verificato} (o che si è realizzato) se l'esito dell'esperimento aleatorio è un elemento di $A$.
\end{definition}

\begin{example}
Consideriamo il lancio di un dado, $\Omega = \{1, 2, 3, 4, 5, 6\}$.
\begin{itemize}
    \item Evento $A = \text{"esce un numero pari"}$: $A = \{2, 4, 6\}$.
    \item Evento $B = \text{"esce un numero maggiore di 4"}$: $B = \{5, 6\}$.
    \item Evento $C = \text{"esce il numero 3"}$: $C = \{3\}$ (questo è un evento elementare).
    \item L'insieme vuoto $\emptyset$ è un evento, chiamato \textbf{evento impossibile} (non può mai verificarsi).
    \item Lo spazio campionario $\Omega$ è un evento, chiamato \textbf{evento certo} (si verifica sempre).
\end{itemize}
\end{example}

\subsection{Operazioni tra Eventi}
Essendo gli eventi degli insiemi, possiamo applicare le usuali operazioni insiemistiche:
\begin{itemize}
    \item \textbf{Unione ($A \cup B$):} L'evento "si verifica $A$ oppure si verifica $B$ (o entrambi)". Corrisponde al connettivo logico "OR".
    \item \textbf{Intersezione ($A \cap B$):} L'evento "si verificano sia $A$ che $B$". Corrisponde al connettivo logico "AND".
    \item \textbf{Complementare ($A^c$ o $\bar{A}$):} L'evento "non si verifica $A$".
\end{itemize}
Due eventi $A$ e $B$ si dicono \textbf{incompatibili} (o \textbf{disgiunti} o \textbf{mutuamente esclusivi}) se non possono verificarsi contemporaneamente, cioè se $A \cap B = \emptyset$.

\begin{example}
Sempre con il lancio del dado:
\begin{itemize}
    \item $A = \{2, 4, 6\}$, $B = \{5, 6\}$.
    \item $A \cup B = \{2, 4, 5, 6\}$ (esce un numero pari OPPURE un numero maggiore di 4).
    \item $A \cap B = \{6\}$ (esce un numero pari E maggiore di 4).
    \item $A^c = \{1, 3, 5\}$ (non esce un numero pari, cioè esce un numero dispari).
    \item Se $D = \{1, 3\}$, allora $A \cap D = \emptyset$, quindi $A$ e $D$ sono incompatibili.
\end{itemize}
\end{example}

\section{Definizione di Probabilità}
Vogliamo assegnare un numero a ciascun evento che ne misuri la "possibilità" di verificarsi.

\begin{definition}[Probabilità]
Dato uno spazio campionario $\Omega$, una \textbf{funzione di probabilità} $\Prob$ è una funzione che assegna a ogni evento $A \subseteq \Omega$ un numero reale $\Prob(A)$ tale che siano soddisfatti i seguenti assiomi (assiomi di Kolmogorov):
\begin{enumerate}
    \item $\Prob(A) \ge 0$ per ogni evento $A$ (non negatività).
    \item $\Prob(\Omega) = 1$ (normalizzazione: l'evento certo ha probabilità 1).
    \item Se $A_1, A_2, \dots, A_n, \dots$ è una successione di eventi a due a due incompatibili (cioè $A_i \cap A_j = \emptyset$ per $i \neq j$), allora
    \[ \Prob\left(\bigcup_{i=1}^{\infty} A_i\right) = \sum_{i=1}^{\infty} \Prob(A_i) \quad (\sigma\text{-additività}) \]
    Se lo spazio campionario $\Omega$ è finito, la $\sigma$-additività si riduce alla \textbf{additività finita}: se $A$ e $B$ sono incompatibili ($A \cap B = \emptyset$), allora $\Prob(A \cup B) = \Prob(A) + \Prob(B)$.
\end{enumerate}
\end{definition}

Da questi assiomi derivano alcune proprietà importanti:
\begin{itemize}
    \item $\Prob(\emptyset) = 0$.
    \item Se $A \subseteq B$, allora $\Prob(A) \le \Prob(B)$.
    \item $0 \le \Prob(A) \le 1$ per ogni evento $A$.
    \item $\Prob(A^c) = 1 - \Prob(A)$.
    \item $\Prob(A \cup B) = \Prob(A) + \Prob(B) - \Prob(A \cap B)$ (principio di inclusione-esclusione per due eventi).
\end{itemize}

\subsection{Spazi Equiprobabili e Calcolo Combinatorio}
In molti problemi, specialmente quelli che coinvolgono dadi, monete "bilanciate", o estrazioni "casuali", si assume che tutti gli esiti elementari in $\Omega$ abbiano la stessa probabilità di verificarsi. Questo è il caso di uno \textbf{spazio campionario equiprobabile} (o uniforme).

Se $\Omega$ è uno spazio campionario finito con $N = |\Omega|$ esiti elementari, e se ogni esito ha la stessa probabilità, allora la probabilità di un singolo esito $\omega_i$ è $\Prob(\{\omega_i\}) = 1/N$.
In questo caso, la probabilità di un evento $A$ è data da:
\[ \Prob(A) = \frac{\text{numero di esiti favorevoli ad } A}{\text{numero di esiti possibili}} = \frac{|A|}{|\Omega|} \]

Qui entra in gioco il \textbf{calcolo combinatorio}, che ci aiuta a contare $|A|$ e $|\Omega|$.
Vediamo alcuni strumenti utili:

\begin{itemize}
    \item \textbf{Disposizioni con Ripetizione ($DR_{n,k}$):} Il numero di modi di scegliere $k$ oggetti da un insieme di $n$ oggetti, con reimmissione e tenendo conto dell'ordine. $DR_{n,k} = n^k$.
    \textit{Esempio:} Numero di sequenze di 3 lanci di una moneta ($n=2$ esiti T/C, $k=3$ lanci): $2^3=8$.
    \textit{Esempio d'esame (Esercizio 1, 22/05/2024):} 5 palline inserite una dopo l'altra in 3 urne. Per ogni pallina ci sono 3 scelte (urna A, B, C). Quindi $|\Omega| = 3^5 = 243$.

    \item \textbf{Permutazioni Semplici ($P_n$):} Il numero di modi di ordinare $n$ oggetti distinti. $P_n = n! = n \cdot (n-1) \cdot \dots \cdot 1$.
    \textit{Esempio:} Numero di modi di anagrammare la parola "TRE": $3! = 6$.

    \item \textbf{Disposizioni Semplici ($D_{n,k}$):} Il numero di modi di scegliere $k$ oggetti da un insieme di $n$ oggetti distinti, senza reimmissione e tenendo conto dell'ordine. $D_{n,k} = n \cdot (n-1) \cdot \dots \cdot (n-k+1) = \frac{n!}{(n-k)!}$.
    \textit{Esempio:} Scegliere un presidente e un tesoriere da 10 persone: $D_{10,2} = 10 \cdot 9 = 90$.

    \item \textbf{Combinazioni Semplici ($C_{n,k}$ o $\binom{n}{k}$):} Il numero di modi di scegliere $k$ oggetti da un insieme di $n$ oggetti distinti, senza reimmissione e \textbf{senza} tener conto dell'ordine.
    \[ \binom{n}{k} = \frac{D_{n,k}}{k!} = \frac{n!}{k!(n-k)!} \]
    \textit{Esempio:} Numero di modi di formare un comitato di 3 persone da un gruppo di 10: $\binom{10}{3}$.
    \textit{Esempio d'esame (Esercizio 2, 08/09/2023):} $|\Omega| = \binom{10}{5}$ è il numero di modi di scegliere i 5 studenti che vanno a Roma (l'altro gruppo è determinato).
    \textit{Esempio d'esame (Esercizio 1, 19/06/2024):} Scegliere 2 lupi da 10 amici. $|\Omega| = \binom{10}{2} = 45$.
\end{itemize}

\begin{example}[Esercizio 2, 08/09/2023 - Studenti biondi a Roma]
\textit{Problema:} Una classe di 10 studenti (4 biondi, 6 non biondi) è divisa in due gruppi di 5, uno per Roma e uno per Firenze. Qual è la probabilità che 2 studenti biondi vadano a Roma?
\textit{Soluzione Passo Passo:}
\begin{enumerate}
    \item \textbf{Definire lo spazio campionario $\Omega$:} L'esito è la composizione del gruppo di 5 studenti che va a Roma. Non conta l'ordine.
    $|\Omega| = \binom{10}{5} = \frac{10!}{5!5!} = \frac{10 \cdot 9 \cdot 8 \cdot 7 \cdot 6}{5 \cdot 4 \cdot 3 \cdot 2 \cdot 1} = 252$.
    Questo è uno spazio equiprobabile.
    \item \textbf{Definire l'evento $B_2 = \text{"2 studenti biondi vanno a Roma"}$}.
    Perché questo evento si verifichi, il gruppo di Roma deve essere formato da:
    \begin{itemize}
        \item 2 studenti biondi (scelti tra i 4 biondi disponibili).
        \item 3 studenti non biondi (scelti tra i 6 non biondi disponibili, per completare il gruppo di 5).
    \end{itemize}
    \item \textbf{Calcolare il numero di esiti favorevoli a $B_2$ ($|B_2|$):}
    Usiamo il principio di moltiplicazione (o delle scelte successive):
    \begin{itemize}
        \item Modi di scegliere 2 biondi da 4: $\binom{4}{2} = \frac{4!}{2!2!} = 6$.
        \item Modi di scegliere 3 non biondi da 6: $\binom{6}{3} = \frac{6!}{3!3!} = \frac{6 \cdot 5 \cdot 4}{3 \cdot 2 \cdot 1} = 20$.
    \end{itemize}
    Quindi, $|B_2| = \binom{4}{2} \cdot \binom{6}{3} = 6 \cdot 20 = 120$.
    \item \textbf{Calcolare la probabilità $\Prob(B_2)$:}
    $\Prob(B_2) = \frac{|B_2|}{|\Omega|} = \frac{120}{252}$.
    Semplificando (dividendo per 12): $\frac{10}{21} \approx 0.476$. (La soluzione dell'esame indica $\approx 0.48$, il che è corretto).
\end{enumerate}
\end{example}

\section{Probabilità Condizionata}
\label{sec:prob_cond}
Spesso siamo interessati alla probabilità di un evento $A$ sapendo che un altro evento $B$ si è già verificato. Questa è la probabilità condizionata.

\begin{definition}[Probabilità Condizionata]
Siano $A$ e $B$ due eventi con $\Prob(B) > 0$. La \textbf{probabilità condizionata} di $A$ dato $B$, indicata con $\Prob(A|B)$, è definita come:
\[ \Prob(A|B) = \frac{\Prob(A \cap B)}{\Prob(B)} \]
Intuizione: $B$ diventa il "nuovo" spazio campionario. Stiamo valutando la probabilità della parte di $A$ che si trova in $B$, normalizzata rispetto alla probabilità di $B$.
\end{definition}

Dalla definizione, segue direttamente la \textbf{Regola della Catena} (o delle probabilità composte):
\[ \Prob(A \cap B) = \Prob(A|B) \Prob(B) \]
Se $\Prob(A)>0$, vale anche $\Prob(A \cap B) = \Prob(B|A) \Prob(A)$.
Per tre eventi $A, B, C$:
\[ \Prob(A \cap B \cap C) = \Prob(A) \Prob(B|A) \Prob(C|A \cap B) \]
e generalizzazioni simili.

\begin{example}[Esercizio 1, 12/01/2024 - Urna di Polya]
\textit{Problema:} Urna con 6 palline (4 Rosse R, 2 Nere N). 3 estrazioni. Quando una pallina è estratta, viene reimbussolata con una nuova pallina dello stesso colore. Calcolare $\Prob(R_3)$, dove $R_3$ è "viene estratta una pallina rossa alla terza estrazione".
\textit{Soluzione (parte di essa, illustrando la regola della catena):}
Consideriamo una sequenza specifica, ad esempio $R_1 \cap N_2 \cap R_3$ (Rossa alla prima, Nera alla seconda, Rossa alla terza).
Usiamo la regola della catena: $\Prob(R_1 \cap N_2 \cap R_3) = \Prob(R_1) \Prob(N_2|R_1) \Prob(R_3|R_1 \cap N_2)$.
\begin{itemize}
    \item $\Prob(R_1)$: Inizialmente 4R, 2N (tot 6). $\Prob(R_1) = 4/6$.
    \item $\Prob(N_2|R_1)$: Se $R_1$ si è verificata, si è aggiunta una R. Urna ora: 5R, 2N (tot 7). $\Prob(N_2|R_1) = 2/7$.
    \item $\Prob(R_3|R_1 \cap N_2)$: Se $R_1$ e $N_2$ si sono verificate, si è aggiunta prima una R, poi una N. Urna ora: 5R, 3N (tot 8). $\Prob(R_3|R_1 \cap N_2) = 5/8$.
\end{itemize}
Quindi, $\Prob(R_1 \cap N_2 \cap R_3) = \frac{4}{6} \cdot \frac{2}{7} \cdot \frac{5}{8}$.
Per calcolare $\Prob(R_3)$ si devono considerare tutte le 8 possibili sequenze che portano a $R_3$ (RR$R_3$, RN$R_3$, NR$R_3$, NN$R_3$) e sommare le loro probabilità (lo vedremo meglio con la formula delle probabilità totali).
\end{example}

\subsection{Indipendenza tra Eventi}
\begin{definition}[Indipendenza tra Eventi]
Due eventi $A$ e $B$ si dicono \textbf{indipendenti} se il verificarsi di uno non influenza la probabilità del verificarsi dell'altro. Formalmente:
\[ \Prob(A \cap B) = \Prob(A) \Prob(B) \]
Se $\Prob(B)>0$, l'indipendenza è equivalente a $\Prob(A|B) = \Prob(A)$.
Se $\Prob(A)>0$, l'indipendenza è equivalente a $\Prob(B|A) = \Prob(B)$.
\end{definition}

\begin{remark}
Non confondere eventi indipendenti con eventi incompatibili!
\begin{itemize}
    \item Se $A, B$ sono incompatibili ($A \cap B = \emptyset$) e $\Prob(A)>0, \Prob(B)>0$, allora $\Prob(A \cap B) = 0$. Ma $\Prob(A)\Prob(B) > 0$. Quindi $A$ e $B$ \textbf{non} sono indipendenti (anzi, sono fortemente dipendenti: se uno si verifica, l'altro non può).
    \item L'unica eccezione è se uno degli eventi ha probabilità zero.
\end{itemize}
\end{remark}

\begin{example}
Lancio di due dadi. $A = \text{"il primo dado è 1"}$, $B = \text{"la somma dei dadi è 7"}$.
$\Omega$ ha $6 \times 6 = 36$ esiti equiprobabili.
$A = \{(1,1), (1,2), (1,3), (1,4), (1,5), (1,6)\}$, $\Prob(A) = 6/36 = 1/6$.
$B = \{(1,6), (2,5), (3,4), (4,3), (5,2), (6,1)\}$, $\Prob(B) = 6/36 = 1/6$.
$A \cap B = \{(1,6)\}$, $\Prob(A \cap B) = 1/36$.
Verifichiamo: $\Prob(A)\Prob(B) = (1/6)(1/6) = 1/36$.
Poiché $\Prob(A \cap B) = \Prob(A)\Prob(B)$, gli eventi $A$ e $B$ sono indipendenti.
\end{example}

\section{Formula delle Probabilità Totali e Teorema di Bayes}
Questi due strumenti sono potentissimi e molto usati negli esercizi.

\begin{definition}[Partizione dello Spazio Campionario]
Una famiglia di eventi $\{B_1, B_2, \dots, B_n\}$ costituisce una \textbf{partizione} di $\Omega$ se:
\begin{enumerate}
    \item $B_i \neq \emptyset$ per ogni $i$.
    \item $B_i \cap B_j = \emptyset$ per ogni $i \neq j$ (sono a due a due incompatibili).
    \item $\bigcup_{i=1}^n B_i = \Omega$ (la loro unione copre tutto lo spazio).
\end{enumerate}
\end{definition}

\begin{theorem}[Formula delle Probabilità Totali]
Sia $\{B_1, B_2, \dots, B_n\}$ una partizione di $\Omega$ tale che $\Prob(B_i) > 0$ per ogni $i$. Allora, per qualsiasi evento $A$:
\[ \Prob(A) = \sum_{i=1}^n \Prob(A \cap B_i) = \sum_{i=1}^n \Prob(A|B_i) \Prob(B_i) \]
\end{theorem}
\textit{Idea:} Per calcolare $\Prob(A)$, si "scompone" $A$ nelle sue parti che intersecano ciascun $B_i$, e si somma la probabilità di queste parti.

\begin{example}[Esercizio 1, 08/09/2023 - Gioco con due monete]
\textit{Problema (Parte 1):} Scommettitore sceglie a caso una di due monete: A (bilanciata, P(Testa)=1/2) o B (truccata, P(Testa)=2/3). Inizia con 1 euro, gioca max 5 volte. Vince 1 euro se Testa, perde 1 euro se Croce. Si ferma se perde tutto (0 euro) o vince 3 euro (ha 4 euro).
Calcolare $\Prob(I_2)$, dove $I_2 = \text{"il gioco si interrompe dopo il secondo lancio"}$.
\textit{Soluzione Passo Passo:}
\begin{enumerate}
    \item \textbf{Identificare la partizione:} L'evento iniziale che condiziona tutto è la scelta della moneta.
    Sia $D_A = \text{"viene scelta la moneta A"}$ e $D_B = \text{"viene scelta la moneta B"}$.
    Poiché la scelta è casuale, $\Prob(D_A) = 1/2$ e $\Prob(D_B) = 1/2$.
    $\{D_A, D_B\}$ è una partizione di $\Omega$ (relativo alla scelta della moneta).
    \item \textbf{Applicare la Formula delle Probabilità Totali a $I_2$:}
    $\Prob(I_2) = \Prob(I_2|D_A)\Prob(D_A) + \Prob(I_2|D_B)\Prob(D_B)$.
    \item \textbf{Analizzare $\Prob(I_2|D_A)$ e $\Prob(I_2|D_B)$:}
    Il giocatore inizia con 1 euro.
    Condizioni di stop: 0 euro o 4 euro.
    Perché il gioco si interrompa AL SECONDO lancio:
    \begin{itemize}
        \item NON deve interrompersi al primo lancio.
        \item DEVE interrompersi al secondo.
    \end{itemize}
    Analizziamo gli stati del capitale del giocatore ($S_0=1$):
    \begin{itemize}
        \item Lancio 1:
            \begin{itemize}
                \item Testa (T1): capitale $S_1=2$. Gioco continua.
                \item Croce (C1): capitale $S_1=0$. Gioco si ferma. Questo è $I_1$.
            \end{itemize}
    \end{itemize}
    Se il gioco arriva al secondo lancio, significa che al primo lancio è uscita Testa (capitale $S_1=2$).
    \begin{itemize}
        \item Lancio 2 (dato T1, quindi $S_1=2$):
            \begin{itemize}
                \item Testa (T2): capitale $S_2=3$. Gioco continua.
                \item Croce (C2): capitale $S_2=1$. Gioco continua.
            \end{itemize}
    \end{itemize}
    In nessuno dei due casi (Testa o Croce al secondo lancio, se si è arrivati al secondo lancio) il giocatore ha 0 euro o 4 euro.
    Quindi, se il secondo lancio viene effettuato, il gioco NON si interrompe dopo il secondo lancio.
    L'unico modo per interrompersi *dopo* il secondo lancio sarebbe se al primo si fosse già interrotto, il che è una contraddizione.
    La soluzione fornita nell'esame afferma: "Si noti che, a prescindere dalla moneta utilizzata, se il secondo lancio viene effettuato, allora al primo lancio c'è stata una vittoria. In questo caso però, il giocatore dispone di 2 euro prima del secondo lancio, e perciò, se anche perdesse al secondo lancio, la sequenza non terminerebbe poiché avrebbe ancora un euro da giocare. Quindi $I_2 = \emptyset \Rightarrow \Prob(I_2) = 0$."
    Questo significa che $\Prob(I_2|D_A) = 0$ e $\Prob(I_2|D_B) = 0$.
    \item \textbf{Calcolare $\Prob(I_2)$:}
    $\Prob(I_2) = 0 \cdot (1/2) + 0 \cdot (1/2) = 0$.
\end{enumerate}
Questo esercizio specifico per $I_2$ era un po' un "trabocchetto" o un test di comprensione delle regole del gioco. Gli altri punti richiedono calcoli più standard.
\end{example}

\begin{theorem}[Teorema di Bayes]
Sia $\{B_1, B_2, \dots, B_n\}$ una partizione di $\Omega$ con $\Prob(B_i) > 0$ per ogni $i$. Sia $A$ un evento con $\Prob(A) > 0$. Allora, per ogni $k=1, \dots, n$:
\[ \Prob(B_k|A) = \frac{\Prob(A|B_k) \Prob(B_k)}{\Prob(A)} = \frac{\Prob(A|B_k) \Prob(B_k)}{\sum_{i=1}^n \Prob(A|B_i) \Prob(B_i)} \]
\end{theorem}
\textit{Idea:} Il teorema di Bayes ci permette di "invertire" la probabilità condizionata. Se conosciamo $\Prob(A|B_k)$ (la probabilità dell'effetto data la causa), possiamo calcolare $\Prob(B_k|A)$ (la probabilità della causa dato l'effetto).
$\Prob(B_k)$ è detta probabilità \textit{a priori} di $B_k$.
$\Prob(B_k|A)$ è detta probabilità \textit{a posteriori} di $B_k$ (dopo aver osservato $A$).

\begin{example}[Esercizio 1, Punto 4, 08/09/2023]
\textit{Problema:} Determinare $\Prob(D_A|P)$, dove $P = \text{"il gioco termina con lo scommettitore che non perde tutti i soldi"}$.
Ricordiamo $D_A = \text{"viene scelta la moneta A"}$.
\textit{Soluzione Passo Passo:}
\begin{enumerate}
    \item \textbf{Applicare il Teorema di Bayes:}
    \[ \Prob(D_A|P) = \frac{\Prob(P|D_A)\Prob(D_A)}{\Prob(P)} \]
    \item \textbf{Scomporre $\Prob(P)$ usando la Formula delle Probabilità Totali:}
    La partizione è $\{D_A, D_B\}$.
    \[ \Prob(P) = \Prob(P|D_A)\Prob(D_A) + \Prob(P|D_B)\Prob(D_B) \]
    Quindi:
    \[ \Prob(D_A|P) = \frac{\Prob(P|D_A)\Prob(D_A)}{\Prob(P|D_A)\Prob(D_A) + \Prob(P|D_B)\Prob(D_B)} \]
    \item \textbf{Utilizzare i valori forniti/calcolati nelle soluzioni dell'esame:}
    Sappiamo $\Prob(D_A) = 1/2$ e $\Prob(D_B) = 1/2$.
    La soluzione dell'esame al punto 3 calcola (anche se con un errore di battitura $D_A$ al posto di $D_B$ nella formula di $P(P^c|D_B)$) e poi fornisce:
    $\Prob(P|D_B) = 136/243$.
    $\Prob(P|D_A) = 5/16$. (Questo è calcolato nel testo della soluzione del punto 4).
    \item \textbf{Sostituire i valori:}
    \[ \Prob(D_A|P) = \frac{(5/16) \cdot (1/2)}{(5/16) \cdot (1/2) + (136/243) \cdot (1/2)} \]
    Si può semplificare il $1/2$:
    \[ \Prob(D_A|P) = \frac{5/16}{5/16 + 136/243} = \frac{5/16}{(5 \cdot 243 + 136 \cdot 16)/(16 \cdot 243)} = \frac{5/16}{(1215 + 2176)/3888} = \frac{5/16}{3391/3888} \]
    \[ = \frac{5}{16} \cdot \frac{3888}{3391} = \frac{5 \cdot 243}{3391} = \frac{1215}{3391} \]
    Questo corrisponde al risultato fornito nella soluzione dell'esame.
\end{enumerate}
\textbf{Come si calcolano $\Prob(P|D_A)$ e $\Prob(P|D_B)$?}
L'evento $P$ è "il gioco termina con lo scommettitore che non perde tutti i soldi". Questo significa che o il giocatore raggiunge i 4 euro, oppure arriva al 5° lancio senza essere andato in rovina e senza aver raggiunto i 4 euro, e termina con un capitale $>0$.
Alternativamente, è più facile calcolare $P^c = \text{"il giocatore perde tutti i soldi"}$, e poi fare $\Prob(P|D_X) = 1 - \Prob(P^c|D_X)$.
Per calcolare $\Prob(P^c|D_X)$ (probabilità di rovina dato che si usa la moneta X), si deve tracciare un albero delle possibili sequenze di gioco e dei capitali, fermandosi quando il capitale è 0 (rovina) o 4 (vittoria) o dopo 5 lanci.
Ad esempio, per $\Prob(P^c|D_A)$ (moneta bilanciata, $p=1/2$ per Testa, $q=1/2$ per Croce):
Capitale iniziale $S_0=1$.
Sequenze di rovina:
\begin{itemize}
    \item C (costo 1/2): $S_1=0$. Rovina. Prob = 1/2.
    \item TC C (costo 1/2, 1/2, 1/2): $S_0=1 \xrightarrow{T} S_1=2 \xrightarrow{C} S_2=1 \xrightarrow{C} S_3=0$. Rovina. Prob = $(1/2)^3 = 1/8$.
    \item TC TC C (costo $(1/2)^5$): $S_0=1 \xrightarrow{T} 2 \xrightarrow{C} 1 \xrightarrow{T} 2 \xrightarrow{C} 1 \xrightarrow{C} 0$. Rovina. Prob = $(1/2)^5 = 1/32$.
    \item TTT C C (costo $(1/2)^5$): $S_0=1 \xrightarrow{T} 2 \xrightarrow{T} 3 \xrightarrow{T} 4$ (STOP VITTORIA). Questa non è rovina.
    \item ... e altre.
\end{itemize}
La soluzione dell'esame usa un approccio che somma le probabilità dei percorsi che portano alla rovina, condizionati alla scelta della moneta.
Per $\Prob(P|D_A) = 1 - \Prob(P^c|D_A)$:
$\Prob(P^c|D_A)$ (rovina con moneta A, $p_A=1/2$):
\begin{itemize}
    \item C1: $1/2$
    \item T1 C2 C3: $(1/2)^3 = 1/8$
    \item T1 C2 T3 C4 C5: $(1/2)^5 = 1/32$
    \item T1 T2 C3 C4 C5: $(1/2)^5 = 1/32$
\end{itemize}
Somma delle probabilità di rovina: $1/2 + 1/8 + 1/32 + 1/32 = 16/32 + 4/32 + 1/32 + 1/32 = 22/32 = 11/16$.
Quindi $\Prob(P|D_A) = 1 - 11/16 = 5/16$. Questo è corretto come nella soluzione.

Per $\Prob(P^c|D_B)$ (rovina con moneta B, $p_B=2/3$ per Testa, $q_B=1/3$ per Croce):
\begin{itemize}
    \item C1: $q_B = 1/3$
    \item T1 C2 C3: $p_B q_B^2 = (2/3)(1/3)^2 = 2/27$
    \item T1 C2 T3 C4 C5: $p_B q_B p_B q_B^2 = p_B^2 q_B^3 = (2/3)^2 (1/3)^3 = 4/27 \cdot 1/27 = 4/243$ (La soluzione dell'esame ha un errore qui, usa $(1/3)^5$ che non ha senso, dovrebbe essere $p_B^2 q_B^3$. No, la soluzione dell'esame è P(C1|DB) + P(T1|DB)P(C2|DB/\text{T1})P(C3|DB/\text{T1}/\text{C2}) + ... che è corretto. Il mio errore era nell'interpretare la formula della soluzione.
    Rivediamo la soluzione dell'esame per $P(P^c|D_B)$:
    \begin{itemize}
        \item $P(C_1|D_B) = 1/3$ (Capitale 0. Rovina)
        \item $P(T_1 \cap C_2 \cap C_3 | D_B) = (2/3)(1/3)(1/3) = 2/27$ (Capitale $1 \to 2 \to 1 \to 0$. Rovina)
        \item $P(T_1 \cap C_2 \cap T_3 \cap C_4 \cap C_5 | D_B) = (2/3)(1/3)(2/3)(1/3)(1/3) = 4/243$ (Capitale $1 \to 2 \to 1 \to 2 \to 1 \to 0$. Rovina)
        \item $P(T_1 \cap T_2 \cap C_3 \cap C_4 \cap C_5 | D_B) = (2/3)(2/3)(1/3)(1/3)(1/3) = 4/243$ (Capitale $1 \to 2 \to 3 \to 2 \to 1 \to 0$. Rovina)
    \end{itemize}
    Sommando: $P(P^c|D_B) = 1/3 + 2/27 + 4/243 + 4/243 = (81+18+4+4)/243 = 107/243$.
    Quindi $\Prob(P|D_B) = 1 - 107/243 = (243-107)/243 = 136/243$. Questo è corretto.
\end{itemize}
La chiave è tracciare attentamente l'albero delle decisioni e dei capitali, fermandosi alle condizioni di stop (0 euro, 4 euro, o 5 lanci).
\end{example}

Questo conclude il primo capitolo sui fondamenti. Dovrebbe darti una base solida. Il prossimo passo sarà introdurre le variabili aleatorie.

% Continuo con la struttura LaTeX di base per i prossimi capitoli.
% Il contenuto dettagliato verrà poi espanso con più esempi dagli esami.

\chapter{Variabili Aleatorie Discrete}
\label{cap:va_discrete}
% ... contenuto del capitolo ...
Una variabile aleatoria (v.a.) è una funzione che associa un numero reale a ogni esito di un esperimento aleatorio. Ci permette di passare da descrizioni qualitative a quantitative.

\begin{definition}[Variabile Aleatoria Discreta]
Una variabile aleatoria $X$ si dice \textbf{discreta} se l'insieme dei valori che può assumere (chiamato \textbf{supporto} $S_X$) è un insieme finito o numerabile (cioè i suoi elementi possono essere messi in corrispondenza biunivoca con i numeri naturali).
\end{definition}

\begin{example}
\begin{itemize}
    \item $X = $ numero di Teste in 3 lanci di una moneta. $S_X = \{0, 1, 2, 3\}$.
    \item $Y = $ risultato del lancio di un dado. $S_Y = \{1, 2, 3, 4, 5, 6\}$.
    \item $Z = $ numero di tentativi fino al primo successo in una serie di prove. $S_Z = \{1, 2, 3, \dots\}$. (Numerabile infinito)
    \item \textbf{Esercizio 3, 08/09/2023:} $X$ è una v.a. con $S_X = \{0, 2, 5\}$.
\end{itemize}
\end{example}

\section{Funzione di Massa di Probabilità (PMF)}
\begin{definition}[Funzione di Massa di Probabilità]
Per una v.a. discreta $X$, la \textbf{funzione di massa di probabilità} (PMF), indicata con $p_X(k)$ o $\Prob(X=k)$, assegna a ogni possibile valore $k$ del supporto $S_X$ la probabilità che $X$ assuma quel valore:
\[ p_X(k) = \Prob(X=k) \]
La PMF deve soddisfare:
\begin{enumerate}
    \item $p_X(k) \ge 0$ per ogni $k \in S_X$.
    \item $\sum_{k \in S_X} p_X(k) = 1$.
    \item $p_X(k) = 0$ se $k \notin S_X$.
\end{enumerate}
\end{definition}

\section{Funzione di Ripartizione (CDF)}
\begin{definition}[Funzione di Ripartizione]
La \textbf{funzione di ripartizione} (CDF) di una v.a. $X$ (discreta o continua), indicata con $F_X(x)$, è definita per ogni $x \in \R$ come:
\[ F_X(x) = \Prob(X \le x) \]
Per una v.a. discreta, $F_X(x) = \sum_{k \le x, k \in S_X} p_X(k)$.
Proprietà della CDF:
\begin{enumerate}
    \item $F_X(x)$ è non decrescente: se $x_1 < x_2$, allora $F_X(x_1) \le F_X(x_2)$.
    \item $\lim_{x \to -\infty} F_X(x) = 0$.
    \item $\lim_{x \to +\infty} F_X(x) = 1$.
    \item $F_X(x)$ è continua da destra: $\lim_{h \to 0^+} F_X(x+h) = F_X(x)$.
    \item Per una v.a. discreta, $F_X(x)$ è una funzione a gradini. I salti avvengono nei punti $k \in S_X$ e l'altezza del salto in $k$ è $p_X(k)$.
    \item $\Prob(a < X \le b) = F_X(b) - F_X(a)$.
    \item $\Prob(X=k) = F_X(k) - F_X(k^-)$ (dove $F_X(k^-) = \lim_{x \to k^-} F_X(x)$).
\end{enumerate}
\end{definition}

\begin{example}[Esercizio 3, Punto 1, 08/09/2023]
$S_X = \{0, 2, 5\}$, $E[X]=2$, $\Var(X)=2$. Determinare la funzione di ripartizione.
\textit{Soluzione Passo Passo:}
\begin{enumerate}
    \item \textbf{Impostare il sistema di equazioni:}
    Siano $p_0 = \Prob(X=0)$, $p_2 = \Prob(X=2)$, $p_5 = \Prob(X=5)$.
    Sappiamo che:
    \begin{itemize}
        \item $p_0 + p_2 + p_5 = 1$ (la somma delle probabilità è 1).
        \item $E[X] = 0 \cdot p_0 + 2 \cdot p_2 + 5 \cdot p_5 = 2p_2 + 5p_5 = 2$.
        \item $\Var(X) = E[X^2] - (E[X])^2 = 2$.
          $E[X^2] = 0^2 p_0 + 2^2 p_2 + 5^2 p_5 = 4p_2 + 25p_5$.
          Quindi, $4p_2 + 25p_5 - (2)^2 = 2 \Rightarrow 4p_2 + 25p_5 - 4 = 2 \Rightarrow 4p_2 + 25p_5 = 6$.
    \end{itemize}
    Abbiamo il sistema:
    \begin{align*}
        p_0 + p_2 + p_5 &= 1 \\
        2p_2 + 5p_5 &= 2 \\
        4p_2 + 25p_5 &= 6
    \end{align*}
    \item \textbf{Risolvere il sistema per $p_2, p_5$ (ultime due equazioni):}
    Dalla seconda: $2p_2 = 2 - 5p_5 \Rightarrow p_2 = 1 - \frac{5}{2}p_5$.
    Sostituisci nella terza: $4(1 - \frac{5}{2}p_5) + 25p_5 = 6$
    $4 - 10p_5 + 25p_5 = 6 \Rightarrow 15p_5 = 2 \Rightarrow p_5 = 2/15$.
    Sostituisci $p_5$ per trovare $p_2$: $p_2 = 1 - \frac{5}{2}(\frac{2}{15}) = 1 - \frac{1}{3} = 2/3$.
    \item \textbf{Trovare $p_0$ dalla prima equazione:}
    $p_0 + 2/3 + 2/15 = 1 \Rightarrow p_0 = 1 - 2/3 - 2/15 = 1 - 10/15 - 2/15 = 1 - 12/15 = 3/15 = 1/5$.
    Quindi, la PMF è: $\Prob(X=0)=1/5$, $\Prob(X=2)=2/3$, $\Prob(X=5)=2/15$.
    Verifica: $1/5 + 2/3 + 2/15 = (3+10+2)/15 = 15/15=1$. Corretto.
    \item \textbf{Scrivere la Funzione di Ripartizione $F_X(x)$:}
    $F_X(x)$ è una funzione a gradini:
    \begin{itemize}
        \item Se $x < 0$, $F_X(x) = \Prob(X \le x) = 0$.
        \item Se $0 \le x < 2$, $F_X(x) = \Prob(X=0) = 1/5$.
        \item Se $2 \le x < 5$, $F_X(x) = \Prob(X=0) + \Prob(X=2) = 1/5 + 2/3 = 3/15 + 10/15 = 13/15$.
        \item Se $x \ge 5$, $F_X(x) = \Prob(X=0) + \Prob(X=2) + \Prob(X=5) = 1/5 + 2/3 + 2/15 = 1$.
    \end{itemize}
    Quindi:
    \[ F_X(x) = \begin{cases} 0 & \text{se } x < 0 \\ 1/5 & \text{se } 0 \le x < 2 \\ 13/15 & \text{se } 2 \le x < 5 \\ 1 & \text{se } x \ge 5 \end{cases} \]
    Questo corrisponde alla soluzione fornita.
\end{enumerate}
\end{example}

\section{Valore Atteso (Media) e Varianza}
\begin{definition}[Valore Atteso]
Il \textbf{valore atteso} (o media) di una v.a. discreta $X$, indicato con $\E[X]$ (o $\mu_X$), è:
\[ \E[X] = \sum_{k \in S_X} k \cdot p_X(k) \]
Rappresenta il valore medio che ci si aspetta di osservare per $X$ su un gran numero di ripetizioni dell'esperimento.
Se $g(X)$ è una funzione di $X$, allora $\E[g(X)] = \sum_{k \in S_X} g(k) \cdot p_X(k)$.
Proprietà (Linearità): $\E[aX+b] = a\E[X]+b$ per costanti $a,b$.
$\E[X+Y] = \E[X]+\E[Y]$ (anche se $X,Y$ non sono indipendenti).
\end{definition}

\begin{definition}[Varianza]
La \textbf{varianza} di una v.a. $X$, indicata con $\Var(X)$ (o $\sigma_X^2$), misura la dispersione dei valori di $X$ attorno alla sua media $\E[X]$:
\[ \Var(X) = \E[(X - \E[X])^2] = \sum_{k \in S_X} (k - \E[X])^2 p_X(k) \]
Una formula computazionalmente più comoda è:
\[ \Var(X) = \E[X^2] - (\E[X])^2 \]
dove $\E[X^2] = \sum_{k \in S_X} k^2 p_X(k)$.
Proprietà:
\begin{itemize}
    \item $\Var(X) \ge 0$.
    \item $\Var(aX+b) = a^2 \Var(X)$ per costanti $a,b$.
    \item Se $X, Y$ sono indipendenti, $\Var(X+Y) = \Var(X) + \Var(Y)$.
\end{itemize}
La \textbf{deviazione standard} $\sigma_X = \sqrt{\Var(X)}$ ha la stessa unità di misura di $X$.
\end{definition}

\section{Principali Distribuzioni Discrete}
Vediamo le distribuzioni che appaiono più frequentemente negli esercizi.

\subsection{Distribuzione di Bernoulli \texorpdfstring{$X \sim \text{Be}(p)$}{X ~ Be(p)}}
Descrive un esperimento con due soli esiti: "successo" (valore 1) e "insuccesso" (valore 0).
$S_X = \{0, 1\}$.
PMF: $\Prob(X=1) = p$, $\Prob(X=0) = 1-p = q$.
$\E[X] = p$.
$\Var(X) = p(1-p) = pq$.

\subsection{Distribuzione Binomiale \texorpdfstring{$X \sim \text{Bin}(n,p)$}{X ~ Bin(n,p)}}
Descrive il numero di successi in $n$ prove di Bernoulli indipendenti, ognuna con probabilità di successo $p$.
$S_X = \{0, 1, \dots, n\}$.
PMF: $\Prob(X=k) = \binom{n}{k} p^k (1-p)^{n-k}$ per $k \in S_X$.
$\E[X] = np$.
$\Var(X) = np(1-p) = npq$.
\begin{example}[Esercizio 3, 19/06/2024 - Palline rosse]
\textit{Problema:} Un'urna inizialmente vuota. Si lancia $n$ volte una moneta (P(Testa)=p). Se esce Testa, si inserisce una pallina rossa, altrimenti verde. $X$ = numero di palline rosse nell'urna dopo $n$ lanci.
\textit{Soluzione:} Ogni lancio è una prova di Bernoulli: successo="esce Testa" (pallina rossa), insuccesso="esce Croce" (pallina verde). Le prove sono indipendenti.
Quindi $X$ segue una distribuzione Binomiale con parametri $n$ (numero di prove/lanci) e $p$ (probabilità di successo/Testa).
$\Prob(X=k) = \binom{n}{k} p^k (1-p)^{n-k}$ per $k \in \{0, 1, \dots, n\}$.
\end{example}

\subsection{Distribuzione Geometrica \texorpdfstring{$X \sim \text{Geo}(p)$}{X ~ Geo(p)}}
Descrive il numero di prove di Bernoulli indipendenti necessarie per ottenere il \textbf{primo} successo.
$S_X = \{1, 2, 3, \dots\}$.
PMF: $\Prob(X=k) = (1-p)^{k-1} p = q^{k-1}p$ per $k \in S_X$.
$\E[X] = 1/p$.
$\Var(X) = (1-p)/p^2 = q/p^2$.
Proprietà di assenza di memoria (per la versione che conta il numero di prove):
$\Prob(X > k+j | X > j) = \Prob(X > k)$.
CDF: $F_X(k) = \Prob(X \le k) = 1 - (1-p)^k = 1 - q^k$. (Questo è mostrato nell'Es. 3, 13/09/2024).

\begin{example}[Esercizio 2, 17/07/2024 - Gioco di Agata]
  % \textit{Problema (parte):} La distanza $T$ della bandierina è una v.a. geometrica di parametro $p=1/2$. Densità discreta $p_T(k) = P(T=k) = p(1-p)^{k-1} = (1/2)(1/2)^{k-1} = (1/2)^k$ per $k \in \N = \{1, 2, \dots\}$.
\textit{Problema (parte):} La distanza $T$ della bandierina è una v.a. geometrica di parametro $p=1/2$. Densità discreta $p_T(k) = P(T=k) = p(1-p)^{k-1} = (1/2)(1/2)^{k-1} = (1/2)^k$ per $k \in \mathbb{N} = \{1, 2, \dots\}$.
\end{example}

\subsection{Distribuzione di Poisson \texorpdfstring{$X \sim \text{Po}(\lambda)$}{X ~ Po(lambda)}}
Descrive il numero di eventi che si verificano in un intervallo di tempo o spazio fissato, quando questi eventi accadono con una frequenza media nota $\lambda$ e indipendentemente l'uno dall'altro.
$S_X = \{0, 1, 2, \dots\}$.
PMF: $\Prob(X=k) = e^{-\lambda} \frac{\lambda^k}{k!}$ per $k \in S_X$.
$\E[X] = \lambda$.
$\Var(X) = \lambda$.
La Binomiale $\text{Bin}(n,p)$ può essere approssimata da una Poisson $\text{Po}(\lambda=np)$ se $n$ è grande e $p$ è piccolo.

\begin{example}[Esercizio 3, Punto 2, 08/09/2023]
$Z$ è una v.a. di Poisson di parametro $\lambda=1$, indipendente da $X$.
Calcolare $\text{cov}(2X+Z, -X+3Z)$.
\textit{Soluzione Passo Passo:}
Ricordiamo la bilinearità della covarianza:
$\Cov(aX+bY, cU+dV) = ac \Cov(X,U) + ad \Cov(X,V) + bc \Cov(Y,U) + bd \Cov(Y,V)$.
Nel nostro caso:
$\Cov(2X+Z, -X+3Z) = \Cov(2X, -X) + \Cov(2X, 3Z) + \Cov(Z, -X) + \Cov(Z, 3Z)$
$= -2 \Cov(X,X) + 6 \Cov(X,Z) - \Cov(Z,X) + 3 \Cov(Z,Z)$
Ricordando che $\Cov(A,A) = \Var(A)$ e $\Cov(A,B) = \Cov(B,A)$:
$= -2 \Var(X) + (6-1) \Cov(X,Z) + 3 \Var(Z)$
$= -2 \Var(X) + 5 \Cov(X,Z) + 3 \Var(Z)$.
Dati del problema:
\begin{itemize}
    \item $\Var(X) = 2$ (calcolato/dato nel punto 1).
    \item $Z \sim \text{Po}(1)$, quindi $\E[Z]=1$ e $\Var(Z)=1$.
    \item $X$ e $Z$ sono indipendenti. Se due v.a. sono indipendenti, la loro covarianza è 0. Quindi $\Cov(X,Z)=0$.
\end{itemize}
Sostituendo:
$\Cov(2X+Z, -X+3Z) = -2(2) + 5(0) + 3(1) = -4 + 0 + 3 = -1$.
Questo corrisponde alla soluzione.
\end{example}

\subsection{Distribuzione Uniforme Discreta \texorpdfstring{$X \sim \text{Unif}\{x_1, \dots, x_n\}$}{X ~ Unif}}
Una v.a. $X$ assume $n$ valori distinti $\{x_1, \dots, x_n\}$ ciascuno con la stessa probabilità $1/n$.
$S_X = \{x_1, \dots, x_n\}$.
PMF: $\Prob(X=x_i) = 1/n$ per $i=1, \dots, n$.
Se $x_i = i$ per $i=1, \dots, n$:
$\E[X] = (n+1)/2$.
$\Var(X) = (n^2-1)/12$.
\begin{example}[Esercizio 1, 17/07/2024 - Lancio due dadi]
$X, Y \sim \text{Unif}\{1, 2, 3, 4, 5, 6\}$. Cioè $P(X=k)=1/6$ per $k \in \{1, ..., 6\}$, e lo stesso per $Y$.
\end{example}

% Continua con gli altri capitoli...
\chapter{Variabili Aleatorie Continue}
\label{cap:va_continue}
% ... contenuto ...
A differenza delle v.a. discrete, una v.a. continua può assumere un'infinità (più che numerabile) di valori in un intervallo. Per una v.a. continua $X$, la probabilità che $X$ assuma un valore specifico $x$ è zero: $\Prob(X=x)=0$. Ha senso invece parlare della probabilità che $X$ cada in un intervallo.

\section{Funzione di Densità di Probabilità (PDF)}
\begin{definition}[Funzione di Densità di Probabilità]
Una funzione $f_X(x)$ è una \textbf{funzione di densità di probabilità} (PDF) per una v.a. continua $X$ se:
\begin{enumerate}
    \item $f_X(x) \ge 0$ per ogni $x \in \R$.
    \item $\int_{-\infty}^{+\infty} f_X(x) \dd x = 1$.
\end{enumerate}
La probabilità che $X$ assuma valori in un intervallo $[a,b]$ è data da:
\[ \Prob(a \le X \le b) = \int_a^b f_X(x) \dd x \]
Nota: Poiché $\Prob(X=x)=0$, allora $\Prob(a \le X \le b) = \Prob(a < X \le b) = \Prob(a \le X < b) = \Prob(a < X < b)$.
\end{definition}

\begin{example}[Esercizio 3, Punto 1, 22/05/2024]
$f_X(x) = \begin{cases} 2x/\theta & \text{se } 0 < x < 3 \\ 0 & \text{altrimenti} \end{cases}$. Determinare $\theta$ affinché $f_X(x)$ sia una densità.
\textit{Soluzione Passo Passo:}
\begin{enumerate}
    \item \textbf{Verificare $f_X(x) \ge 0$:}
    Poiché $x \in (0,3)$, $2x > 0$. Affinché $2x/\theta \ge 0$, e dato che $\theta>0$ è specificato nel testo, questa condizione è soddisfatta.
    \item \textbf{Imporre l'integrale unitario:}
    \[ \int_{-\infty}^{+\infty} f_X(x) \dd x = 1 \]
    L'integrale è non nullo solo tra 0 e 3:
    \[ \int_0^3 \frac{2x}{\theta} \dd x = \frac{2}{\theta} \int_0^3 x \dd x = \frac{2}{\theta} \left[ \frac{x^2}{2} \right]_0^3 = \frac{2}{\theta} \left( \frac{3^2}{2} - \frac{0^2}{2} \right) = \frac{2}{\theta} \cdot \frac{9}{2} = \frac{9}{\theta} \]
    Imponiamo che questo sia uguale a 1:
    \[ \frac{9}{\theta} = 1 \Rightarrow \theta = 9 \]
    Questo corrisponde alla soluzione.
\end{enumerate}
\end{example}

\section{Funzione di Ripartizione (CDF)}
La definizione di CDF è la stessa vista per le v.a. discrete: $F_X(x) = \Prob(X \le x)$.
Per una v.a. continua con PDF $f_X(t)$:
\[ F_X(x) = \int_{-\infty}^x f_X(t) \dd t \]
Proprietà:
\begin{itemize}
    \item Le stesse 4 proprietà viste per le discrete (non decrescente, limiti 0 e 1, continua da destra).
    \item Per una v.a. continua, $F_X(x)$ è una funzione continua (non solo da destra).
    \item Se $F_X(x)$ è derivabile, allora $f_X(x) = \frac{\dd}{\dd x} F_X(x)$ nei punti di continuità di $f_X(x)$.
\end{itemize}

\begin{example}[Esercizio 3, Punto 2, 22/05/2024]
Data $f_X(x)$ con $\theta=9$, determinare $F_X(x)$.
$f_X(x) = \begin{cases} 2x/9 & \text{se } 0 < x < 3 \\ 0 & \text{altrimenti} \end{cases}$.
\textit{Soluzione Passo Passo:}
Dobbiamo calcolare $F_X(x) = \int_{-\infty}^x f_X(t) \dd t$ per diversi intervalli di $x$.
\begin{itemize}
    \item \textbf{Se $x \le 0$:}
    $F_X(x) = \int_{-\infty}^x 0 \dd t = 0$.
    \item \textbf{Se $0 < x < 3$:}
    $F_X(x) = \int_{-\infty}^0 0 \dd t + \int_0^x \frac{2t}{9} \dd t = 0 + \frac{2}{9} \left[ \frac{t^2}{2} \right]_0^x = \frac{2}{9} \left( \frac{x^2}{2} - 0 \right) = \frac{x^2}{9}$.
    \item \textbf{Se $x \ge 3$:}
    $F_X(x) = \int_{-\infty}^0 0 \dd t + \int_0^3 \frac{2t}{9} \dd t + \int_3^x 0 \dd t = 0 + \frac{2}{9} \left[ \frac{t^2}{2} \right]_0^3 + 0 = \frac{2}{9} \left( \frac{3^2}{2} - 0 \right) = \frac{9}{9} = 1$.
\end{itemize}
Quindi:
\[ F_X(x) = \begin{cases} 0 & \text{se } x \le 0 \\ x^2/9 & \text{se } 0 < x < 3 \\ 1 & \text{se } x \ge 3 \end{cases} \]
Questo corrisponde alla soluzione (notare che la soluzione dell'esame ha $x \le 0$ e $x \ge 3$ come casi, il che è corretto, ma per $x=0$ e $x=3$ le formule $x^2/9$ danno gli stessi valori di $0$ e $1$ rispettivamente, rendendo la funzione continua).
\end{example}

\section{Valore Atteso e Varianza}
\begin{definition}[Valore Atteso]
Il \textbf{valore atteso} (o media) di una v.a. continua $X$ con PDF $f_X(x)$ è:
\[ \E[X] = \int_{-\infty}^{+\infty} x f_X(x) \dd x \]
Se $g(X)$ è una funzione di $X$, allora $\E[g(X)] = \int_{-\infty}^{+\infty} g(x) f_X(x) \dd x$.
Le proprietà sono le stesse del caso discreto.
\end{definition}

\begin{definition}[Varianza]
La \textbf{varianza} di una v.a. continua $X$ è:
\[ \Var(X) = \E[(X - \E[X])^2] = \int_{-\infty}^{+\infty} (x - \E[X])^2 f_X(x) \dd x \]
E la formula computazionale:
\[ \Var(X) = \E[X^2] - (\E[X])^2 \]
dove $\E[X^2] = \int_{-\infty}^{+\infty} x^2 f_X(x) \dd x$.
Le proprietà sono le stesse del caso discreto.
\end{definition}

\begin{example}[Esercizio 3, Punto 3, 22/05/2024]
Calcolare $\Prob(1 \le X \le 2)$ e $\E[X]$ per la $X$ dell'esempio precedente.
\textit{Soluzione Passo Passo:}
\begin{enumerate}
    \item \textbf{Calcolare $\Prob(1 \le X \le 2)$:}
    Possiamo usare la PDF o la CDF.
    Con la PDF: $\Prob(1 \le X \le 2) = \int_1^2 f_X(x) \dd x = \int_1^2 \frac{2x}{9} \dd x = \frac{2}{9} \left[ \frac{x^2}{2} \right]_1^2 = \frac{2}{9} \left( \frac{4}{2} - \frac{1}{2} \right) = \frac{2}{9} \cdot \frac{3}{2} = \frac{3}{9} = \frac{1}{3}$.
    Con la CDF: $\Prob(1 \le X \le 2) = F_X(2) - F_X(1)$.
    $F_X(2) = 2^2/9 = 4/9$.
    $F_X(1) = 1^2/9 = 1/9$.
    $F_X(2) - F_X(1) = 4/9 - 1/9 = 3/9 = 1/3$.
    Entrambi i metodi danno lo stesso risultato, che corrisponde alla soluzione.
    \item \textbf{Calcolare $\E[X]$:}
    $\E[X] = \int_{-\infty}^{+\infty} x f_X(x) \dd x = \int_0^3 x \left(\frac{2x}{9}\right) \dd x = \int_0^3 \frac{2x^2}{9} \dd x$
    $= \frac{2}{9} \left[ \frac{x^3}{3} \right]_0^3 = \frac{2}{9} \left( \frac{3^3}{3} - 0 \right) = \frac{2}{9} \cdot \frac{27}{3} = \frac{2}{9} \cdot 9 = 2$.
    Corrisponde alla soluzione.
\end{enumerate}
\end{example}

\section{Principali Distribuzioni Continue}

\subsection{Distribuzione Uniforme Continua \texorpdfstring{$X \sim \text{Unif}(a,b)$}{X ~ Unif(a,b)}}
Una v.a. $X$ ha la stessa probabilità di assumere valori in qualsiasi sottointervallo di uguale lunghezza all'interno di $[a,b]$.
PDF: $f_X(x) = \begin{cases} \frac{1}{b-a} & \text{se } a \le x \le b \\ 0 & \text{altrimenti} \end{cases}$.
CDF: $F_X(x) = \begin{cases} 0 & \text{se } x < a \\ \frac{x-a}{b-a} & \text{se } a \le x \le b \\ 1 & \text{se } x > b \end{cases}$.
$\E[X] = (a+b)/2$.
$\Var(X) = (b-a)^2/12$.
\begin{example}[Esercizio 3, 17/07/2024]
$X \sim \text{Unif}(0,2)$.
Quindi $a=0, b=2$.
PDF: $f_X(x) = 1/(2-0) = 1/2$ per $0 \le x \le 2$, e 0 altrove.
$\E[X] = (0+2)/2 = 1$.
$\Var(X) = (2-0)^2/12 = 4/12 = 1/3$.
\end{example}

\subsection{Distribuzione Esponenziale \texorpdfstring{$X \sim \text{Exp}(\lambda)$}{X ~ Exp(lambda)}}
Spesso usata per modellare tempi di attesa o durate di vita, con $\lambda > 0$.
PDF: $f_X(x) = \begin{cases} \lambda e^{-\lambda x} & \text{se } x \ge 0 \\ 0 & \text{se } x < 0 \end{cases}$.
CDF: $F_X(x) = \begin{cases} 1 - e^{-\lambda x} & \text{se } x \ge 0 \\ 0 & \text{se } x < 0 \end{cases}$.
$\E[X] = 1/\lambda$.
$\Var(X) = 1/\lambda^2$.
Proprietà di \textbf{assenza di memoria}: $\Prob(X > s+t | X > s) = \Prob(X > t)$ per $s, t \ge 0$.
(Significa che se un oggetto è "sopravvissuto" fino al tempo $s$, la probabilità che sopravviva per un ulteriore tempo $t$ è la stessa che un oggetto nuovo sopravviva per un tempo $t$).

\begin{example}[Esercizio 4, 12/01/2024 - Tempo di attesa casse]
\textit{Problema (parte):} Cassa A, tempo $T_A \sim \text{Exp}(0.2)$. Cassa B, tempo $T_B \sim \text{Exp}(0.5)$. Il cliente sceglie la cassa a caso (prob 1/2 ciascuna). $T$ è il tempo d'attesa.
Determinare $F_T(t)$.
\textit{Soluzione Passo Passo:}
Sia $C_A = \text{"sceglie cassa A"}$, $C_B = \text{"sceglie cassa B"}$. $\Prob(C_A)=\Prob(C_B)=1/2$.
Usiamo la Formula delle Probabilità Totali per la CDF:
$F_T(t) = \Prob(T \le t) = \Prob(T \le t | C_A)\Prob(C_A) + \Prob(T \le t | C_B)\Prob(C_B)$.
Se sceglie A, $T=T_A$, quindi $\Prob(T \le t | C_A) = F_{T_A}(t) = 1 - e^{-0.2t}$ (per $t \ge 0$).
Se sceglie B, $T=T_B$, quindi $\Prob(T \le t | C_B) = F_{T_B}(t) = 1 - e^{-0.5t}$ (per $t \ge 0$).
Quindi, per $t \ge 0$:
$F_T(t) = (1 - e^{-0.2t}) \cdot \frac{1}{2} + (1 - e^{-0.5t}) \cdot \frac{1}{2}$
$= \frac{1}{2} - \frac{1}{2}e^{-0.2t} + \frac{1}{2} - \frac{1}{2}e^{-0.5t} = 1 - \frac{e^{-0.2t} + e^{-0.5t}}{2}$.
Per $t < 0$, $F_T(t)=0$.
Questo corrisponde alla soluzione.
\end{example}

\subsection{Distribuzione Normale (Gaussiana) \texorpdfstring{$X \sim N(\mu, \sigma^2)$}{X ~ N(mu, sigma^2)}}
È una delle distribuzioni più importanti e diffuse. Caratterizzata da media $\mu$ e varianza $\sigma^2$.
PDF: $f_X(x) = \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}$ per $x \in \R$.
$\E[X] = \mu$.
$\Var(X) = \sigma^2$.
La CDF non ha una forma chiusa semplice. Ci si riferisce alla \textbf{Normale Standard} $Z \sim N(0,1)$, la cui CDF è indicata con $\Phi(z) = \Prob(Z \le z)$.
Per una $X \sim N(\mu, \sigma^2)$, si può \textbf{standardizzare}: $Z = \frac{X-\mu}{\sigma} \sim N(0,1)$.
Quindi $\Prob(X \le x) = \Prob\left(\frac{X-\mu}{\sigma} \le \frac{x-\mu}{\sigma}\right) = \Prob\left(Z \le \frac{x-\mu}{\sigma}\right) = \Phi\left(\frac{x-\mu}{\sigma}\right)$.
Proprietà di $\Phi(z)$:
\begin{itemize}
    \item $\Phi(-z) = 1 - \Phi(z)$.
    \item $\Prob(a < Z < b) = \Phi(b) - \Phi(a)$.
    \item Se $z_\alpha$ è tale che $\Prob(Z > z_\alpha) = \alpha$, allora $\Phi(z_\alpha)=1-\alpha$, e $z_\alpha = \Phi^{-1}(1-\alpha)$.
    \item Se $\Prob(Z < z_\beta) = \beta$, allora $z_\beta = \Phi^{-1}(\beta)$. Dalla simmetria, $\Phi^{-1}(\beta) = -\Phi^{-1}(1-\beta)$.
\end{itemize}

\begin{example}[Esercizio 4, 08/09/2023 - Temperatura]
\textit{Problema (parte):} Temperatura $T \sim N(\mu=20, \sigma^2=4)$. Quindi $\sigma=\sqrt{4}=2$.
$N$ (numero lettini) dipende da $T$: $N=0$ se $T<17$, $N=20$ se $17 \le T < 22$, ecc.
Determinare $\E[N]$.
\textit{Soluzione Passo Passo (per $\E[N]$):}
$\E[N] = 0 \cdot \Prob(T<17) + 20 \cdot \Prob(17 \le T < 22) + 50 \cdot \Prob(22 \le T < 24) + 100 \cdot \Prob(T \ge 24)$.
Dobbiamo calcolare queste probabilità. Standardizziamo $Z = (T-20)/2$.
\begin{itemize}
    \item $E_1 = \{T<17\}$:
    $\Prob(T<17) = \Prob\left(Z < \frac{17-20}{2}\right) = \Prob(Z < -1.5) = \Phi(-1.5)$.
    La soluzione usa $\approx 0.067$.
    \item $E_2 = \{17 \le T < 22\}$:
    $\Prob(17 \le T < 22) = \Prob\left(\frac{17-20}{2} \le Z < \frac{22-20}{2}\right) = \Prob(-1.5 \le Z < 1)$
    $= \Phi(1) - \Phi(-1.5)$. La soluzione usa $\approx 0.774$.
    \item $E_3 = \{22 \le T < 24\}$:
    $\Prob(22 \le T < 24) = \Prob\left(\frac{22-20}{2} \le Z < \frac{24-20}{2}\right) = \Prob(1 \le Z < 2)$
    $= \Phi(2) - \Phi(1)$. La soluzione usa $\approx 0.136$.
    \item $E_4 = \{T \ge 24\}$:
    $\Prob(T \ge 24) = \Prob\left(Z \ge \frac{24-20}{2}\right) = \Prob(Z \ge 2) = 1 - \Prob(Z < 2) = 1 - \Phi(2)$.
    La soluzione usa $\approx 0.023$.
\end{itemize}
Somma delle probabilità (verifica): $0.067 + 0.774 + 0.136 + 0.023 = 1$. Corretto.
$\E[N] = 0 \cdot (0.067) + 20 \cdot (0.774) + 50 \cdot (0.136) + 100 \cdot (0.023)$
$= 0 + 15.48 + 6.8 + 2.3 = 24.58$. Corrisponde alla soluzione.
\end{example}

\begin{example}[Esercizio 4, Punto 3, 08/09/2023 - Soglia di temperatura]
Determinare $t$ tale che $\Prob(T > t) \ge 0.70$. Viene dato $\Phi^{-1}(0.7) \approx 0.5244$.
\textit{Soluzione Passo Passo:}
$\Prob(T > t) = 0.70$ (prendiamo il caso limite).
$\Prob\left(Z > \frac{t-20}{2}\right) = 0.70$.
Questo significa $1 - \Prob\left(Z \le \frac{t-20}{2}\right) = 0.70$.
$1 - \Phi\left(\frac{t-20}{2}\right) = 0.70 \Rightarrow \Phi\left(\frac{t-20}{2}\right) = 1 - 0.70 = 0.30$.
Quindi $\frac{t-20}{2} = \Phi^{-1}(0.30)$.
Usiamo la proprietà $\Phi^{-1}(\beta) = -\Phi^{-1}(1-\beta)$:
$\Phi^{-1}(0.30) = -\Phi^{-1}(1-0.30) = -\Phi^{-1}(0.70)$.
Dato $\Phi^{-1}(0.70) \approx 0.5244$, allora $\Phi^{-1}(0.30) \approx -0.5244$.
$\frac{t-20}{2} = -0.5244 \Rightarrow t-20 = 2(-0.5244) = -1.0488$.
$t = 20 - 1.0488 = 18.9512$.
Questo corrisponde alla soluzione. La condizione $\Prob(T > t) \ge 0.70$ implica che $t$ deve essere al massimo questo valore, quindi $t \le 18.9512$. La domanda chiede "soglia massima", ma il contesto suggerisce "valore $t$ tale che la probabilità di superarlo sia 0.7".
\end{example}


\chapter{Variabili Aleatorie Congiunte}
\label{cap:va_congiunte}
% ... contenuto ...
Spesso siamo interessati a studiare due o più variabili aleatorie definite sullo stesso esperimento.

\section{Caso Discreto}
\begin{definition}[Funzione di Massa di Probabilità Congiunta]
Date due v.a. discrete $X$ e $Y$, la loro \textbf{funzione di massa di probabilità congiunta} è:
\[ p_{X,Y}(x,y) = \Prob(X=x, Y=y) \]
Proprietà:
\begin{enumerate}
    \item $p_{X,Y}(x,y) \ge 0$.
    \item $\sum_x \sum_y p_{X,Y}(x,y) = 1$.
\end{enumerate}
\end{definition}

\begin{definition}[Distribuzioni Marginali]
Dalla PMF congiunta, possiamo ottenere le PMF delle singole variabili (marginali):
\[ p_X(x) = \Prob(X=x) = \sum_y p_{X,Y}(x,y) \]
\[ p_Y(y) = \Prob(Y=y) = \sum_x p_{X,Y}(x,y) \]
\end{definition}

\begin{definition}[Distribuzioni Condizionate]
La PMF condizionata di $Y$ dato $X=x$ (con $p_X(x)>0$) è:
\[ p_{Y|X}(y|x) = \Prob(Y=y|X=x) = \frac{\Prob(X=x, Y=y)}{\Prob(X=x)} = \frac{p_{X,Y}(x,y)}{p_X(x)} \]
Similmente per $p_{X|Y}(x|y)$.
\end{definition}

\section{Caso Continuo}
\begin{definition}[Funzione di Densità di Probabilità Congiunta]
Date due v.a. continue $X$ e $Y$, la loro \textbf{funzione di densità di probabilità congiunta} $f_{X,Y}(x,y)$ è tale che:
\begin{enumerate}
    \item $f_{X,Y}(x,y) \ge 0$.
    \item $\int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} f_{X,Y}(x,y) \dd x \dd y = 1$.
    \item $\Prob((X,Y) \in A) = \iint_A f_{X,Y}(x,y) \dd x \dd y$ per $A \subseteq \R^2$.
\end{enumerate}
\end{definition}

\begin{definition}[Distribuzioni Marginali]
\[ f_X(x) = \int_{-\infty}^{+\infty} f_{X,Y}(x,y) \dd y \]
\[ f_Y(y) = \int_{-\infty}^{+\infty} f_{X,Y}(x,y) \dd x \]
\end{definition}

\begin{definition}[Distribuzioni Condizionate]
La PDF condizionata di $Y$ dato $X=x$ (con $f_X(x)>0$) è:
\[ f_{Y|X}(y|x) = \frac{f_{X,Y}(x,y)}{f_X(x)} \]
Similmente per $f_{X|Y}(x|y)$.
\end{definition}

\section{Indipendenza tra Variabili Aleatorie}
\begin{definition}[Indipendenza]
Due v.a. $X$ e $Y$ sono \textbf{indipendenti} se per ogni $x,y$:
\begin{itemize}
    \item Caso discreto: $p_{X,Y}(x,y) = p_X(x) p_Y(y)$.
    \item Caso continuo: $f_{X,Y}(x,y) = f_X(x) f_Y(y)$.
\end{itemize}
Equivalentemente, $F_{X,Y}(x,y) = F_X(x) F_Y(y)$.
Se $X,Y$ sono indipendenti, allora $\E[XY] = \E[X]\E[Y]$.
Se $X,Y$ sono indipendenti, allora $\E[g(X)h(Y)] = \E[g(X)]\E[h(Y)]$.
\end{definition}

\begin{example}[Esercizio 3, Punto 1, 12/01/2024 - Completare tabella]
\textit{Problema:} Completare la tabella di probabilità congiunta in modo che $X, Y$ siano indipendenti.
\begin{center}
\begin{tabular}{c|ccc|c}
$Y \setminus X$ & -2 & 0 & 3 & $p_X(x)$ \\ \hline
-1 & $p(-1,-2)$ & 0.2 & $p(-1,3)$ & $p_X(-1)$ \\
3 & 0.1 & $p(3,0)$ & $p(3,3)$ & $p_X(3)$ \\ \hline
$p_Y(y)$ & $p_Y(-2)$ & 0.4 & $p_Y(3)$ & 1
\end{tabular}
\end{center}
Valori noti: $p(-1,0)=0.2$, $p(3,-2)=0.1$, $p_Y(0)=0.4$.
\textit{Soluzione Passo Passo:}
\begin{enumerate}
    \item \textbf{Usare la complementarità per $p_{X,Y}(3,0)$:}
    La colonna di $Y=0$ deve sommare a $p_Y(0)=0.4$.
    $p(-1,0) + p(3,0) = p_Y(0) \Rightarrow 0.2 + p(3,0) = 0.4 \Rightarrow p(3,0) = 0.2$.
    \item \textbf{Usare l'indipendenza $p(x,y) = p_X(x)p_Y(y)$:}
    Da $p(3,0) = 0.2$: $p(3,0) = p_X(3) \cdot p_Y(0) \Rightarrow 0.2 = p_X(3) \cdot 0.4 \Rightarrow p_X(3) = 0.2/0.4 = 0.5$.
    \item \textbf{Calcolare $p_X(-1)$:}
    Le probabilità marginali $p_X(x)$ devono sommare a 1.
    $p_X(-1) + p_X(3) = 1 \Rightarrow p_X(-1) + 0.5 = 1 \Rightarrow p_X(-1) = 0.5$.
    \item \textbf{Usare l'indipendenza per $p(-1,0)$:}
    $p(-1,0) = p_X(-1) \cdot p_Y(0) \Rightarrow 0.2 = 0.5 \cdot 0.4 \Rightarrow 0.2 = 0.2$. (Consistente!)
    \item \textbf{Usare l'indipendenza per $p(3,-2)$:}
    $p(3,-2) = p_X(3) \cdot p_Y(-2) \Rightarrow 0.1 = 0.5 \cdot p_Y(-2) \Rightarrow p_Y(-2) = 0.1/0.5 = 0.2$.
    \item \textbf{Calcolare $p_Y(3)$:}
    Le probabilità marginali $p_Y(y)$ devono sommare a 1.
    $p_Y(-2) + p_Y(0) + p_Y(3) = 1 \Rightarrow 0.2 + 0.4 + p_Y(3) = 1 \Rightarrow 0.6 + p_Y(3) = 1 \Rightarrow p_Y(3) = 0.4$.
    \item \textbf{Completare le celle rimanenti usando l'indipendenza:}
    $p(-1,-2) = p_X(-1)p_Y(-2) = 0.5 \cdot 0.2 = 0.1$.
    $p(-1,3) = p_X(-1)p_Y(3) = 0.5 \cdot 0.4 = 0.2$.
    $p(3,3) = p_X(3)p_Y(3) = 0.5 \cdot 0.4 = 0.2$.
\end{enumerate}
Tabella completata:
\begin{center}
\begin{tabular}{c|ccc|c}
$Y \setminus X$ & -2 & 0 & 3 & $p_X(x)$ \\ \hline
-1 & \textbf{0.1} & 0.2 & \textbf{0.2} & \textbf{0.5} \\
3 & 0.1 & \textbf{0.2} & \textbf{0.2} & \textbf{0.5} \\ \hline
$p_Y(y)$ & \textbf{0.2} & 0.4 & \textbf{0.4} & 1
\end{tabular}
\end{center}
Questo corrisponde alla tabella nella soluzione.
\end{example}

\section{Covarianza e Correlazione}
\begin{definition}[Covarianza]
La \textbf{covarianza} tra due v.a. $X$ e $Y$ misura la loro tendenza a variare insieme:
\[ \Cov(X,Y) = \E[(X-\E[X])(Y-\E[Y])] \]
Formula computazionale:
\[ \Cov(X,Y) = \E[XY] - \E[X]\E[Y] \]
dove $\E[XY] = \sum_x \sum_y xy \cdot p_{X,Y}(x,y)$ (discreto) o $\iint xy f_{X,Y}(x,y) \dd x \dd y$ (continuo).
Proprietà:
\begin{itemize}
    \item $\Cov(X,X) = \Var(X)$.
    \item $\Cov(X,Y) = \Cov(Y,X)$.
    \item $\Cov(aX+b, cY+d) = ac \Cov(X,Y)$.
    \item $\Var(X+Y) = \Var(X) + \Var(Y) + 2\Cov(X,Y)$.
    \item Se $X, Y$ sono indipendenti, allora $\Cov(X,Y) = 0$. \textbf{Attenzione:} $\Cov(X,Y)=0$ non implica indipendenza, a meno che $X,Y$ non siano Normali.
\end{itemize}
\end{definition}

\begin{definition}[Coefficiente di Correlazione]
Il \textbf{coefficiente di correlazione lineare} $\rho_{X,Y}$ normalizza la covarianza:
\[ \rho_{X,Y} = \frac{\Cov(X,Y)}{\sqrt{\Var(X)\Var(Y)}} = \frac{\Cov(X,Y)}{\sigma_X \sigma_Y} \]
Proprietà:
\begin{itemize}
    \item $-1 \le \rho_{X,Y} \le 1$.
    \item Se $\rho_{X,Y} = 1$, c'è una relazione lineare positiva perfetta ($Y=aX+b$ con $a>0$).
    \item Se $\rho_{X,Y} = -1$, c'è una relazione lineare negativa perfetta ($Y=aX+b$ con $a<0$).
    \item Se $\rho_{X,Y} = 0$, non c'è correlazione lineare (ma potrebbe esserci una dipendenza non lineare).
    \item Se $X,Y$ sono indipendenti, $\rho_{X,Y}=0$.
\end{itemize}
\end{definition}
\begin{example}[Esercizio 3, Punto 4, 08/09/2023]
Affinché $\rho_{X,Y}=1$ deve valere $Y = f(X) = aX+b$ con $a>0$.
Date le condizioni $\Var(Y) = \E[Y]=2$ e $\E[X]=2, \Var(X)=2$.
Usiamo le proprietà:
$\E[Y] = \E[aX+b] = a\E[X]+b = 2a+b$.
$\Var(Y) = \Var(aX+b) = a^2\Var(X) = a^2(2)$.
Quindi:
$2a+b = 2$
$2a^2 = 2 \Rightarrow a^2 = 1$. Poiché $a>0$, si ha $a=1$.
Sostituendo $a=1$ nella prima: $2(1)+b=2 \Rightarrow b=0$.
Quindi $f(x) = x$. Questo corrisponde alla soluzione.
\end{example}

\chapter{Catene di Markov}
\label{cap:markov}
Una catena di Markov è un modello matematico per descrivere sistemi che evolvono nel tempo tra un insieme discreto di stati, dove la probabilità di transizione al prossimo stato dipende solo dallo stato attuale e non dalla sequenza di eventi che lo hanno preceduto (proprietà di Markov).

\section{Definizioni e Notazioni}
\begin{itemize}
    \item \textbf{Spazio degli Stati $S$}: L'insieme (finito o numerabile) di tutti i possibili stati del sistema. Es: $S=\{0, 1, 2\}$ o $S=\{A, B, C, D\}$.
    \item \textbf{Tempo Discreto}: Osserviamo il sistema a istanti $n=0, 1, 2, \dots$.
    \item \textbf{$X_n$}: La variabile aleatoria che rappresenta lo stato del sistema al tempo $n$.
    \item \textbf{Proprietà di Markov}:
    \[ \Prob(X_{n+1}=j | X_n=i, X_{n-1}=i_{n-1}, \dots, X_0=i_0) = \Prob(X_{n+1}=j | X_n=i) \]
    \item \textbf{Probabilità di Transizione (omogenee nel tempo)}: $p_{ij} = \Prob(X_{n+1}=j | X_n=i)$. Non dipendono da $n$.
    \item \textbf{Matrice di Transizione $P$}: Una matrice quadrata le cui entrate sono $p_{ij}$.
    \[ P = [p_{ij}]_{i,j \in S} \]
    Proprietà di $P$:
    \begin{itemize}
        \item $p_{ij} \ge 0$ per ogni $i,j$.
        \item $\sum_{j \in S} p_{ij} = 1$ per ogni riga $i$ (da ogni stato si deve transire da qualche parte).
    \end{itemize}
    \item \textbf{Grafo della Catena}: Un grafo orientato dove i nodi sono gli stati e un arco da $i$ a $j$ esiste se $p_{ij}>0$, etichettato con $p_{ij}$.
    \item \textbf{Probabilità di Transizione a n-passi $p_{ij}^{(n)}$}: $p_{ij}^{(n)} = \Prob(X_n=j | X_0=i)$.
    La matrice $P^{(n)}$ delle probabilità di transizione a $n$-passi è $P^{(n)} = P \cdot P \cdot \dots \cdot P = P^n$ (prodotto di matrici).
\end{itemize}

\begin{example}[Esercizio 4, 22/05/2024 - Genotipi Conigli]
\textit{Problema (parte a):} Stati: 0 (gg), 1 (gG), 2 (GG). $X_{n-1}$ è il genotipo di un genitore, l'altro è sempre un ibrido (gG, stato 1). $X_n$ è il genotipo della prole. Determinare la matrice di transizione.
\textit{Soluzione Passo Passo (calcolo di una riga della matrice):}
Consideriamo $p_{0j} = \Prob(X_n=j | X_{n-1}=0)$. Cioè, genitore 1 è 'gg', genitore 2 è 'gG'.
Ogni genitore dà un allele (g o G) con probabilità 1/2 (se ibrido) o 1 (se omozigote).
\begin{itemize}
    \item Genitore 1 (gg) dà sempre 'g'.
    \item Genitore 2 (gG) dà 'g' con prob 1/2, 'G' con prob 1/2.
\end{itemize}
Prole:
\begin{itemize}
    \item $X_n=0$ (gg): si ottiene se Genitore 1 dà 'g' E Genitore 2 dà 'g'. Prob = $1 \cdot (1/2) = 1/2$. Quindi $p_{00}=1/2$.
    \item $X_n=1$ (gG): si ottiene se Genitore 1 dà 'g' E Genitore 2 dà 'G'. Prob = $1 \cdot (1/2) = 1/2$. Quindi $p_{01}=1/2$.
    \item $X_n=2$ (GG): impossibile. Prob = 0. Quindi $p_{02}=0$.
\end{itemize}
Prima riga di $P$: $[1/2, 1/2, 0]$.
Analogamente si calcolano le altre righe. La soluzione fornisce:
\[ P = \begin{pmatrix} 1/2 & 1/2 & 0 \\ 1/4 & 1/2 & 1/4 \\ 0 & 1/2 & 1/2 \end{pmatrix} \]
Il grafo si disegna con 3 nodi (0,1,2) e gli archi pesati con le $p_{ij}$ non nulle.
\end{example}

\section{Classificazione degli Stati}
\begin{itemize}
    \item Uno stato $j$ è \textbf{accessibile} da $i$ ($i \to j$) se $p_{ij}^{(n)} > 0$ per qualche $n \ge 0$.
    \item Due stati $i,j$ \textbf{comunicano} ($i \leftrightarrow j$) se $i \to j$ e $j \to i$. La comunicazione è una relazione di equivalenza e partiziona $S$ in \textbf{classi comunicanti}.
    \item Una catena è \textbf{irriducibile} se ha una sola classe comunicante (tutti gli stati comunicano tra loro).
    \item Uno stato $i$ è \textbf{ricorrente} se, partendo da $i$, la probabilità di ritornare in $i$ è 1. Altrimenti è \textbf{transiente}.
    \begin{itemize}
        \item In una classe comunicante finita, tutti gli stati sono o tutti ricorrenti o tutti transienti.
        \item Se una classe è chiusa (non si può uscire) e finita, allora è ricorrente.
    \end{itemize}
    \item Uno stato $i$ è \textbf{assorbente} se $p_{ii}=1$ (una volta entrati, non si esce più). Uno stato assorbente forma una classe comunicante ricorrente da solo.
    \item Il \textbf{periodo} $d(i)$ di uno stato $i$ è il MCD$\{n \ge 1 : p_{ii}^{(n)} > 0\}$. Se $d(i)=1$, lo stato è \textbf{aperiodico}.
    \begin{itemize}
        \item Tutti gli stati nella stessa classe comunicante hanno lo stesso periodo.
        \item Se una catena irriducibile ha almeno un $p_{ii}>0$ (cappio nel grafo), allora è aperiodica.
    \end{itemize}
\end{itemize}

\section{Distribuzione Stazionaria (Invariante)}
\begin{definition}[Distribuzione Stazionaria]
Un vettore riga di probabilità $\pi = (\pi_0, \pi_1, \dots)$ è una \textbf{distribuzione stazionaria} (o invariante) se soddisfa:
\[ \pi P = \pi \quad \text{e} \quad \sum_i \pi_i = 1 \]
Se la distribuzione iniziale degli stati è $\pi$, allora anche la distribuzione al tempo successivo sarà $\pi$. Rappresenta l'equilibrio a lungo termine della catena.
\end{definition}

\begin{theorem}[Teorema Ergodico]
Se una catena di Markov è \textbf{irriducibile} e \textbf{aperiodica} (e ha spazio degli stati finito), allora:
\begin{enumerate}
    \item Esiste un'unica distribuzione stazionaria $\pi$.
    \item Per ogni stato iniziale $i$ e ogni stato $j$, $\lim_{n \to \infty} p_{ij}^{(n)} = \pi_j$.
    (Le righe della matrice $P^n$ convergono tutte al vettore $\pi$).
\end{enumerate}
Se la catena è irriducibile ma periodica, il limite sopra non esiste, ma esiste ancora un'unica distribuzione stazionaria.
\end{theorem}

\begin{example}[Esercizio 4, Punto b, 22/05/2024 - Conigli]
Determinare la distribuzione invariante $\pi = (\pi_0, \pi_1, \pi_2)$ per $P = \begin{pmatrix} 1/2 & 1/2 & 0 \\ 1/4 & 1/2 & 1/4 \\ 0 & 1/2 & 1/2 \end{pmatrix}$.
\textit{Soluzione Passo Passo:}
\begin{enumerate}
    \item \textbf{Impostare il sistema $\pi P = \pi$:}
    \begin{align*}
        \pi_0 &= \frac{1}{2}\pi_0 + \frac{1}{4}\pi_1 + 0\pi_2 \\
        \pi_1 &= \frac{1}{2}\pi_0 + \frac{1}{2}\pi_1 + \frac{1}{2}\pi_2 \\
        \pi_2 &= 0\pi_0 + \frac{1}{4}\pi_1 + \frac{1}{2}\pi_2
    \end{align*}
    E la condizione di normalizzazione: $\pi_0 + \pi_1 + \pi_2 = 1$.
    \item \textbf{Semplificare le equazioni:}
    (1) $\frac{1}{2}\pi_0 = \frac{1}{4}\pi_1 \Rightarrow 2\pi_0 = \pi_1$.
    (3) $\frac{1}{2}\pi_2 = \frac{1}{4}\pi_1 \Rightarrow 2\pi_2 = \pi_1$.
    (Notare che la (2) diventa: $\frac{1}{2}\pi_1 = \frac{1}{2}\pi_0 + \frac{1}{2}\pi_2 \Rightarrow \pi_1 = \pi_0 + \pi_2$. Se sostituiamo $\pi_0=\pi_1/2$ e $\pi_2=\pi_1/2$, otteniamo $\pi_1 = \pi_1/2 + \pi_1/2 \Rightarrow \pi_1=\pi_1$, quindi la (2) è linearmente dipendente dalle altre due).
    \item \textbf{Usare la normalizzazione:}
    Da $2\pi_0 = \pi_1$ e $2\pi_2 = \pi_1$, abbiamo $\pi_0 = \pi_1/2$ e $\pi_2 = \pi_1/2$.
    Sostituiamo in $\pi_0 + \pi_1 + \pi_2 = 1$:
    $\frac{\pi_1}{2} + \pi_1 + \frac{\pi_1}{2} = 1 \Rightarrow 2\pi_1 = 1 \Rightarrow \pi_1 = 1/2$.
    \item \textbf{Trovare $\pi_0$ e $\pi_2$:}
    $\pi_0 = \pi_1/2 = (1/2)/2 = 1/4$.
    $\pi_2 = \pi_1/2 = (1/2)/2 = 1/4$.
    Quindi $\pi = (1/4, 1/2, 1/4)$. Questo corrisponde alla soluzione.
    \item \textbf{Cosa si può dire del genotipo alla centesima generazione?}
    La catena è irriducibile (tutti gli stati comunicano) e aperiodica (ad es. $p_{11}=1/2 > 0$, quindi $d(1)=1$). Per il teorema ergodico, la distribuzione degli stati dopo molti passi converge a $\pi$. Quindi, alla centesima generazione, la probabilità che il coniglio sia gg è $\approx \pi_0=1/4$, gG è $\approx \pi_1=1/2$, GG è $\approx \pi_2=1/4$.
\end{enumerate}
\end{example}

\section{Probabilità di Assorbimento}
Se una catena ha uno o più stati assorbenti, siamo spesso interessati alla probabilità che, partendo da uno stato transiente $i$, la catena venga assorbita in un particolare stato assorbente $j$.
Sia $h_i$ la probabilità di essere assorbiti in un certo stato (o insieme di stati) assorbente $S_A$, partendo dallo stato $i$.
Si imposta un sistema di equazioni:
\begin{itemize}
    \item $h_i = 1$ se $i \in S_A$ (se parto da uno stato assorbente, ci sono già).
    \item $h_i = 0$ se $i$ è uno stato assorbente che NON è in $S_A$ (se l'obiettivo è essere assorbiti in A, ma parto da un altro stato assorbente B, non potrò mai raggiungere A).
    \item Per gli stati transienti $i$: $h_i = \sum_{k \in S} p_{ik} h_k$. (Condizionando sul primo passo).
\end{itemize}
\begin{example}[Esercizio 4, Punto c, 17/07/2024 - Jack's game]
\textit{Problema:} Jack ha 2 euro, vuole arrivare a 8 euro (chiamata). Strategia 1: scommette 2 euro. P(vince)=0.4, P(perde)=0.6. Stati: 0, 2, 4, 6, 8 (euro). 0 e 8 sono assorbenti.
Qual è la probabilità di arrivare a 8 partendo da 2 (ovvero $h_2^8$)?
Matrice di transizione (solo stati rilevanti per calcolare $h_i^8$):
Stati: 0 (assorbente, non vince), 2 (transiente), 4 (transiente), 6 (transiente), 8 (assorbente, vince)
$P = \begin{pmatrix}
1 & 0 & 0 & 0 & 0 \\ % da 0
p_{20} & 0 & p_{24} & 0 & 0 \\ % da 2 (perde va a 0, vince va a 4)
0 & p_{42} & 0 & p_{46} & 0 \\ % da 4
0 & 0 & p_{64} & 0 & p_{68} \\ % da 6
0 & 0 & 0 & 0 & 1    % da 8
\end{pmatrix}$
$p_{i, i-2} = 0.6$ (perde 2), $p_{i, i+2} = 0.4$ (vince 2).
Quindi $p_{20}=0.6, p_{24}=0.4$; $p_{42}=0.6, p_{46}=0.4$; $p_{64}=0.6, p_{68}=0.4$.
Sistema per $h_i = \Prob(\text{raggiungere 8 prima di 0, partendo da i})$:
\begin{itemize}
    \item $h_0 = 0$ (partendo da 0 non si raggiunge 8 come prima cosa).
    \item $h_8 = 1$ (partendo da 8 si è già raggiunto).
    \item $h_2 = p_{20}h_0 + p_{24}h_4 = 0.6(0) + 0.4h_4 = 0.4h_4$.
    \item $h_4 = p_{42}h_2 + p_{46}h_6 = 0.6h_2 + 0.4h_6$.
    \item $h_6 = p_{64}h_4 + p_{68}h_8 = 0.6h_4 + 0.4(1) = 0.6h_4 + 0.4$.
\end{itemize}
Sostituiamo:
$h_4 = 0.6(0.4h_4) + 0.4(0.6h_4 + 0.4)$
$h_4 = 0.24h_4 + 0.24h_4 + 0.16$
$h_4 = 0.48h_4 + 0.16$
$h_4(1 - 0.48) = 0.16 \Rightarrow 0.52h_4 = 0.16 \Rightarrow h_4 = 0.16/0.52 = 16/52 = 4/13$.
Allora $h_2 = 0.4 h_4 = (2/5)(4/13) = 8/65$.
$h_6 = 0.6(4/13) + 0.4 = (12/65) + (26/65) = 38/65$. (Errore nella soluzione data dell'esame, $h_6^8$ è $38/65 \approx 0.58$, non $4/25$).
La soluzione fornita nell'esame ha $h_2^8 = 8/65 \approx 0.1231$. Questo è il valore corretto per $h_2$.
La soluzione dice $h_4^8 = 4/13$, $h_6^8 = 8/25$ (questo sembra un errore, dovrebbe essere $h_6 = 0.6(4/13)+0.4 = (2.4/13)+0.4 = (12/65) + (26/65) = 38/65$).
Ma il calcolo di $h_2^8=8/65$ è corretto.

Per la strategia 2: Stati 0, 2, 4, 8. $P(\text{vince})=0.4, P(\text{perde})=0.6$. Scommette tutto.
\begin{itemize}
    \item Da 2: scommette 2. Vince $\to$ 4 (prob 0.4). Perde $\to$ 0 (prob 0.6).
    \item Da 4: scommette 4. Vince $\to$ 8 (prob 0.4). Perde $\to$ 0 (prob 0.6).
\end{itemize}
Sistema per $h_i = \Prob(\text{raggiungere 8 prima di 0, partendo da i})$:
\begin{itemize}
    \item $h_0 = 0$.
    \item $h_8 = 1$.
    \item $h_2 = p_{20}h_0 + p_{24}h_4 = 0.6(0) + 0.4h_4 = 0.4h_4$.
    \item $h_4 = p_{40}h_0 + p_{48}h_8 = 0.6(0) + 0.4(1) = 0.4$.
\end{itemize}
$h_4 = 0.4$.
$h_2 = 0.4 h_4 = 0.4(0.4) = 0.16 = 4/25$.
Confrontando $h_2$: Strategia 1 dà $8/65 \approx 0.123$. Strategia 2 dà $0.16$.
La strategia 2 è migliore, come indicato nella soluzione.
\end{example}


\chapter{Strategie per Risolvere gli Esercizi e Riepilogo}
% ... contenuto ...
\begin{enumerate}
    \item \textbf{Leggi attentamente il testo}: Identifica cosa viene chiesto, quali sono i dati e quali sono le variabili aleatorie coinvolte.
    \item \textbf{Definisci lo Spazio Campionario $\Omega$ e gli Eventi di interesse}: Se possibile, conta gli elementi e verifica se lo spazio è equiprobabile.
    \item \textbf{Applica le formule base}: Unione, intersezione, complementare, probabilità condizionata.
    \item \textbf{Usa Formula delle Probabilità Totali e Bayes} quando hai una partizione dello spazio e probabilità condizionate.
    \item \textbf{Identifica le Variabili Aleatorie e le loro Distribuzioni}:
        \begin{itemize}
            \item È discreta o continua? Qual è il supporto?
            \item Riconosci una distribuzione nota (Binomiale, Poisson, Geometrica, Uniforme, Esponenziale, Normale)? Se sì, usa le sue proprietà (PMF/PDF, E[X], Var(X)).
            \item Se non è nota, calcola PMF/PDF, CDF, E[X], Var(X) dalle definizioni.
        \end{itemize}
    \item \textbf{Per Variabili Congiunte}:
        \begin{itemize}
            \item Scrivi la tabella (discreto) o la funzione (continuo) di probabilità/densità congiunta.
            \item Calcola marginali e condizionate se necessario.
            \item Verifica l'indipendenza.
            \item Calcola covarianza e correlazione.
        \end{itemize}
    \item \textbf{Per Catene di Markov}:
        \begin{itemize}
            \item Identifica gli stati e scrivi la matrice di transizione $P$. Disegna il grafo.
            \item Classifica gli stati (irriducibilità, aperiodicità, ricorrenza/transitorietà, assorbimento).
            \item Se richiesto, calcola $P^{(n)}$.
            \item Se la catena è irriducibile e aperiodica (e finita), calcola la distribuzione stazionaria $\pi$ risolvendo $\pi P = \pi$ e $\sum \pi_i = 1$.
            \item Se ci sono stati assorbenti, calcola le probabilità di assorbimento $h_i$.
        \end{itemize}
    \item \textbf{Controlla i risultati}: Le probabilità devono essere tra 0 e 1. Le varianze non negative. Le somme delle probabilità su tutto il supporto devono fare 1.
\end{enumerate}

\appendix
\chapter{Tabelle e Formulario Essenziale}
\section{Principali Distribuzioni Discrete}
\begin{tabular}{|l|c|c|c|c|}
\hline
Distribuzione & Notazione & PMF $\Prob(X=k)$ & $\E[X]$ & $\Var(X)$ \\ \hline
Bernoulli & Be$(p)$ & $p^k(1-p)^{1-k}$, $k \in \{0,1\}$ & $p$ & $p(1-p)$ \\ \hline
Binomiale & Bin$(n,p)$ & $\binom{n}{k}p^k(1-p)^{n-k}$, $k \in \{0,..,n\}$ & $np$ & $np(1-p)$ \\ \hline
Geometrica & Geo$(p)$ & $(1-p)^{k-1}p$, $k \in \{1,2,..\}$ & $1/p$ & $(1-p)/p^2$ \\ \hline
Poisson & Po$(\lambda)$ & $e^{-\lambda}\frac{\lambda^k}{k!}$, $k \in \{0,1,..\}$ & $\lambda$ & $\lambda$ \\ \hline
Unif. Discr. & U$\{x_1,..,x_N\}$ & $1/N$, $k \in \{x_1,..,x_N\}$ & $\frac{1}{N}\sum x_i$ & $\frac{1}{N}\sum (x_i-\mu)^2$ \\ \hline
\end{tabular}

\section{Principali Distribuzioni Continue}
\begin{tabular}{|l|c|c|c|c|}
\hline
Distribuzione & Notazione & PDF $f_X(x)$ & $\E[X]$ & $\Var(X)$ \\ \hline
Uniforme & Unif$(a,b)$ & $\frac{1}{b-a}$ per $a \le x \le b$ & $\frac{a+b}{2}$ & $\frac{(b-a)^2}{12}$ \\ \hline
Esponenziale & Exp$(\lambda)$ & $\lambda e^{-\lambda x}$ per $x \ge 0$ & $1/\lambda$ & $1/\lambda^2$ \\ \hline
Normale & N$(\mu, \sigma^2)$ & $\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$ & $\mu$ & $\sigma^2$ \\ \hline
\end{tabular}

\section{Formule Utili}
\begin{itemize}
    \item $\Prob(A^c) = 1 - \Prob(A)$
    \item $\Prob(A \cup B) = \Prob(A) + \Prob(B) - \Prob(A \cap B)$
    \item Prob. Condizionata: $\Prob(A|B) = \Prob(A \cap B) / \Prob(B)$
    \item Indipendenza: $\Prob(A \cap B) = \Prob(A)\Prob(B)$
    \item Prob. Totali: $\Prob(A) = \sum_i \Prob(A|B_i)\Prob(B_i)$ (per partizione $B_i$)
    \item Bayes: $\Prob(B_k|A) = \Prob(A|B_k)\Prob(B_k) / \Prob(A)$
    \item $\Var(X) = \E[X^2] - (\E[X])^2$
    \item $\Cov(X,Y) = \E[XY] - \E[X]\E[Y]$
    \item $\Var(aX+bY) = a^2\Var(X) + b^2\Var(Y) + 2ab\Cov(X,Y)$
\end{itemize}
(Qui andrebbe inserita una piccola tavola della Normale Standard o un riferimento a come usarla con una calcolatrice/software).

\end{document}